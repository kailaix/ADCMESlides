%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%    PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass[usenames,dvipsnames]{beamer}
\usepackage{animate}
\usepackage{float}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{extarrows}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{minted}

\newcommand{\ChoL}{\mathsf{L}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\ii}{\mathrm{i}}
\newcommand{\bxi}{\bm{\xi}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bM}{\mathbf{M}}

\newcommand{\by}{\mathbf{y}}
\newcommand{\bw}{\mathbf{w}}

\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\bt}[0]{\bm{\theta}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bzero}{\mathbf{0}}
\renewcommand{\bf}{\mathbf{f}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}[0]{\mathbf{v}}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}


% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{soul}
\newcommand{\red}[1]{\textcolor{red}{#1}}
%
%\usepackage{graphicx} % Allows including images
%\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
%
%
%\usepackage{amsthm}
%
%\usepackage{todonotes}
%\usepackage{floatrow}
%
%\usepackage{pgfplots,algorithmic,algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
%\usepackage[toc,page]{appendix}
%\usepackage{float}
%\usepackage{booktabs}
%\usepackage{bm}
%
%\theoremstyle{definition}
%
\newcommand{\RR}[0]{\mathbb{R}}
%
%\newcommand{\bx}{\mathbf{x}}
%\newcommand{\ii}{\mathrm{i}}
%\newcommand{\bxi}{\bm{\xi}}
%\newcommand{\bmu}{\bm{\mu}}
%\newcommand{\bb}{\mathbf{b}}
%\newcommand{\bA}{\mathbf{A}}
%\newcommand{\bJ}{\mathbf{J}}
%\newcommand{\bB}{\mathbf{B}}
%\newcommand{\bM}{\mathbf{M}}
%\newcommand{\bF}{\mathbf{F}}
%
%\newcommand{\by}{\mathbf{y}}
%\newcommand{\bw}{\mathbf{w}}
%\newcommand{\bn}{\mathbf{n}}
%
%\newcommand{\bX}{\mathbf{X}}
%\newcommand{\bY}{\mathbf{Y}}
%\newcommand{\bs}{\mathbf{s}}
%\newcommand{\sign}{\mathrm{sign}}
%\newcommand{\bt}[0]{\bm{\theta}}
%\newcommand{\bc}{\mathbf{c}}
%\newcommand{\bzero}{\mathbf{0}}
%\renewcommand{\bf}{\mathbf{f}}
%\newcommand{\bu}{\mathbf{u}}
%\newcommand{\bv}[0]{\mathbf{v}}

\AtBeginSection[]
{
   \begin{frame}
       \frametitle{Outline}
       \tableofcontents[currentsection]
   \end{frame}
}

%----------------------------------------------------------------------------------------
%    TITLE PAGE
%----------------------------------------------------------------------------------------
\usepackage{bm}
\newcommand*{\TakeFourierOrnament}[1]{{%
\fontencoding{U}\fontfamily{futs}\selectfont\char#1}}
\newcommand*{\danger}{\TakeFourierOrnament{66}}

\title[ML for Computational Engineering]{Machine Learning for Inverse Problems in Computational Engineering} % The short title appears at the bottom of every slide, the full title is only on the title page

\author[ADCME]{Kailai Xu and Eric Darve \\ \url{https://github.com/kailaix/ADCME.jl}} % Your name
%\institute[] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
%{
%%ICME, Stanford University \\ % Your institution for the title page
%%\medskip
%%\textit{kailaix@stanford.edu}\quad \textit{darve@stanford.edu} % Your email address
%}
\date{}% Date, can be changed to a custom date
% Mathematics of PDEs


\begin{document}

\usebackgroundtemplate{%
\begin{picture}(0,250)
\centering
	{{\includegraphics[width=1.0\paperwidth]{figures/background}}}
\end{picture}
  } 
%\usebackgroundtemplate{%
%  \includegraphics[width=\paperwidth,height=\paperheight]{figures/back}} 
\begin{frame}

\titlepage % Print the title page as the first slide

%dfa
\end{frame}
\usebackgroundtemplate{}

\section{Inverse Modeling}




\begin{frame}
	\frametitle{Inverse Modeling}
	\begin{figure}
		\centering
		\includegraphics[width=1.0\textwidth]{figures/inverse3}
	\end{figure}
\end{frame}


\begin{frame}
	\frametitle{Inverse Modeling}
	\begin{itemize}
		\item \textbf{Inverse modeling} identifies a certain set of parameters or functions with which the outputs of the forward analysis matches the desired result or measurement.
		\item Many real life engineering problems can be formulated as inverse modeling problems: shape optimization for improving the performance of structures, optimal control of fluid dynamic systems, etc.
	\end{itemize}
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/inverse2}
	\end{figure}
\end{frame}




\begin{frame}
	\frametitle{Inverse Modeling}
	We can formulate inverse modeling as a PDE-constrained optimization problem 
	\begin{equation*}
		\min_{\theta} L_h(u_h) \quad \mathrm{s.t.}\; F_h(\theta, u_h) = 0
	\end{equation*}
	\begin{itemize}
		\item The \textcolor{red}{loss function} $L_h$ measures the discrepancy between the prediction $u_h$ and the observation $u_{\mathrm{obs}}$, e.g., $L_h(u_h) = \|u_h - u_{\mathrm{obs}}\|_2^2$. 
		\item $\theta$ is the \textcolor{red}{model parameter} to be calibrated. 
		\item The \textcolor{red}{physics constraints} $F_h(\theta, u_h)=0$ are described by a system of partial differential equations or differential algebraic equations (DAEs); e.g., 
		$$F_h(\theta, u_h) = \mathbf{A}(\theta) u_h - f_h = 0$$
	\end{itemize}
\end{frame}




\begin{frame}
	\frametitle{Function Inverse Problem}
	
	\begin{equation*}
		\min_{\textcolor{red}{f}} L_h(u_h) \quad \mathrm{s.t.}\; F_h(\textcolor{red}{f}, u_h) = 0
	\end{equation*}
	
	What if the unknown is a \textcolor{red}{function} instead of a set of parameters?
\begin{itemize}
	\item Koopman operator in dynamical systems.
	\item Constitutive relations in solid mechanics. 
	\item Turbulent closure relations in fluid mechanics.
	\item ...
\end{itemize}

The candidate solution space is \textcolor{red}{infinite dimensional}.

\end{frame}


\begin{frame}
	\frametitle{Penalty Methods}
	
	\begin{itemize}
		\item Parametrize $f$ with $f_\theta$ and incorporate the physical constraint as a \textcolor{red}{penalty term} (regularization, prior, \ldots) in the loss function.
		\begin{equation*}
	\min_{\theta,\,u_h} L_h(u_h) + \lambda\|F_h(f_\theta, u_h)\|_2^2
		\end{equation*}
		\begin{itemize}
			\item May not satisfying physical constraint $F_h(f_\theta, u_h)=0$ accurately;
			\item Slow convergence for \textcolor{red}{stiff} problems;
			\begin{figure}[hbt]
				\includegraphics[width=0.6\textwidth]{figures/slowconvergence.png}
			\end{figure}
			
			
			\item High dimensional optimization problem; both $\theta$ and $u_h$ are variables.
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Machine Learning for Computational Engineering}
	$$\min_{\theta} L_h(u_h) \quad \mathrm{s.t.}\;\boxed{F_h(\textcolor{red}{NN_\theta}, u_h) = 0} \leftarrow \mbox{Solved numerically}$$
	\vspace{-0.5cm}
	\begin{itemize}
		\item Deep neural networks exhibit capability of approximating high dimensional and complicated functions. 
		\item \textbf{Machine Learning for Computational Engineering}: \textcolor{red}{the unknown function is approximated by a deep neural network, and the physical constraints are enforced by numerical schemes}.
		\item \textcolor{red}{Satisfy the physics to the largest extent}.
	\end{itemize}
	\begin{figure}[hbt]
  \includegraphics[width=0.75\textwidth]{figures/physics_based_machine_learning.png}
\end{figure}
\end{frame}



\begin{frame}
	\frametitle{Gradient Based Optimization}
	\begin{equation}\label{equ:opt}
		\min_{\theta} L_h(u_h) \quad \mathrm{s.t.}\; F_h(\theta, u_h) = 0
		\end{equation}
	
	\begin{itemize}
		\item We can now apply a gradient-based optimization method to (\ref{equ:opt}).
		\item The key is to \textcolor{red}{calculate the gradient descent direction} $g^k$
		$$\theta^{k+1} \gets \theta^k - \alpha g^k$$ 
	\end{itemize}
	
	\begin{figure}[hbt]
	\centering
  \includegraphics[width=0.6\textwidth]{figures/im.pdf}
\end{figure}

\end{frame}



\section{Automatic Differentiation}


\begin{frame}
	\frametitle{Automatic Differentiation}
	The fact that bridges the \textcolor{red}{technical} gap between machine learning and inverse modeling:
	\begin{itemize}
		\item Deep learning (and many other machine learning techniques) and numerical schemes share the same computational model: composition of individual operators. 
	\end{itemize}
	
	
	\begin{minipage}[t]{0.4\textwidth}
		
		\
		
		
		
		\begin{center}
			\textcolor{red}{Mathematical Fact}
			
			\
			
			Back-propagation 
			
			$||$
			
			Reverse-mode
			
			Automatic Differentiation 
			
			$||$
			
			Discrete 
			
			Adjoint-State Method
		\end{center}
	\end{minipage}~
	\begin{minipage}[t]{0.6\textwidth}
		\begin{figure}[hbt]
			\includegraphics[width=0.8\textwidth]{figures/compare-NN-PDE.png}
		\end{figure}
	\end{minipage}
	
\end{frame}

\begin{frame}
	\frametitle{Automatic Differentiation: Forward-mode and Reverse-mode}
	\begin{figure}
		\centering
		\includegraphics[width=1.0\textwidth]{figures/forwardreverse}
	\end{figure}
\end{frame}



\begin{frame}
	\frametitle{What is the Appropriate Model for Inverse Problems?}
	
	\begin{itemize}
		\item In general, for a function $f:\RR^n \rightarrow \RR^m$
		% Please add the following required packages to your document preamble:
		% \usepackage{booktabs}
		\begin{table}[]
			\centering
			\begin{tabular}{@{}llll@{}}
				\toprule
				Mode & Suitable for ... & Complexity\footnote{$\mathrm{OPS}$ is a metric for complexity in terms of fused-multiply adds.} & Application \\ \midrule
				Forward & $m\gg n$ & $\leq 2.5\;\mathrm{OPS}(f(x))$ & UQ \\
				Reverse & $m\ll n$ & $\leq 4\;\mathrm{OPS}(f(x))$ & Inverse Modeling \\ \bottomrule
			\end{tabular}
		\end{table}
		
		
		\item There are also many other interesting topics
		\begin{itemize}
			\item Mixed mode AD: many-to-many mappings.
			\item Computing sparse Jacobian matrices using AD by exploiting sparse structures. 
		\end{itemize}
	\end{itemize}
	{\scriptsize Margossian CC. A review of automatic differentiation and its efficient implementation. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery. 2019 Jul;9(4):e1305.} 
\end{frame}


\begin{frame}
	\frametitle{Computational Graph for Numerical Schemes}
	
	\begin{itemize}
		\item To leverage automatic differentiation for inverse modeling, we need to express the numerical schemes in the ``AD language'': computational graph. 
		\item No matter how complicated a numerical scheme is, it can be decomposed into a collection of operators that are interlinked via state variable dependencies. 
	\end{itemize}
	
	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/cgnum}
	\end{figure}
	
	
	
\end{frame}


\begin{frame}
	\frametitle{ADCME: Computational-Graph-based Numerical Simulation}
	
	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/custom}
	\end{figure}
\end{frame}

\begin{frame}
\frametitle{How ADCME works}
\begin{itemize}
	\item ADCME translates your numerical simulation codes to computational graph and then the computations are delegated to a heterogeneous task-based parallel computing environment through TensorFlow runtime. 
\end{itemize}
\begin{figure}[hbt]
	\includegraphics[width=1.0\textwidth]{figures/routine2.png}
\end{figure}
\end{frame}


\begin{frame}
	\frametitle{Granularity of Automatic Differentiation}
	
	\begin{itemize}
		\item Coarser granularity gives researchers more control over gradient back-propagation. 
	\end{itemize}
	
	
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.75\textwidth]{figures/adlevel}
	\end{figure}
	
	
	
\end{frame}

\section{Physics Constrained Learning}

\begin{frame}
	
	
	\frametitle{Challenges in AD}
	
	
	\begin{minipage}{0.695\textwidth}
		\vspace{-6cm}
		\begin{itemize}
			\item Most AD frameworks only deal with \textcolor{red}{explicit operators}, i.e., the functions that has analytical derivatives, or composition of these functions. 
			\item Many scientific computing algorithms are \textcolor{red}{iterative} or \textcolor{red}{implicit} in nature.
			\vspace{1cm}
			{\small
			\begin{table}[]
				\begin{tabular}{@{}lll@{}}
					\toprule
					Linear/Nonlinear & Explicit/Implicit & Expression   \\ \midrule
					Linear           & Explicit          & $y=Ax$       \\
					Nonlinear        & Explicit          & $y = F(x)$   \\
					\textbf{Linear}           & \textbf{Implicit}          & $Ay = x$     \\
					\textbf{Nonlinear}        & \textbf{Implicit}          & $F(x,y) = 0$ \\ \bottomrule
				\end{tabular}
			\end{table}
		}
		\end{itemize}
	\end{minipage}~
	\begin{minipage}[t]{0.3\textwidth}
		\includegraphics[width=1.0\textwidth]{figures/implicitvsexplicit.png}
	\end{minipage}
	
	% Please add the following required packages to your document preamble:
	% \usepackage{booktabs}
	
\end{frame}

\begin{frame}
	\frametitle{Example}
	
	\begin{itemize}
		\item Consider a function $f:x\rightarrow y$, which is implicitly defined by 
		$$F(x,y) = x^3 - (y^3+y) = 0$$
		If not using the cubic formula for finding the roots, the forward computation consists of iterative algorithms, such as the Newton's method and bisection method
	\end{itemize}
	
	
	
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\begin{algorithmic}
			\State $y^0 \gets 0$
			\State $k \gets 0$
			\While {$|F(x, y^k)|>\epsilon$}
			\State $\delta^k \gets F(x, y^k)/F'_y(x,y^k)$
			\State $y^{k+1}\gets y^k - \delta^k$
			\State $k \gets k+1$
			\EndWhile
			\State \textbf{Return} $y^k$
		\end{algorithmic}
	\end{minipage}~
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\begin{algorithmic}
			\State $l \gets -M$, $r\gets M$, $m\gets 0$
			\While {$|F(x, m)|>\epsilon$}
			\State $c \gets \frac{a+b}{2}$
			\If{$F(x, m)>0$}
			\State $a\gets m$
			\Else
			\State $b\gets m$
			\EndIf
			\EndWhile
			\State \textbf{Return} $c$
		\end{algorithmic}
		
	\end{minipage}	
	
\end{frame}

\begin{frame}
	\frametitle{Example}
	
	\begin{itemize}
		%		\item A simple approach is to save part or all intermediate steps, and ``back-propagate''. This approach is expensive in both computation and memory\footnote{Ablin, Pierre, Gabriel Peyr�, and Thomas Moreau. ``Super-efficiency of automatic differentiation for functions defined as a minimum.''}.
		%		\item Nevertheless, the simple approach works in some scenarios where accuracy or cost is not an issue, e.g., automatic differetiation of soft-DTW and Sinkhorn distance. 
		\item An efficient way to do automatic differentiation is to apply the \textcolor{red}{implicit function theorem}. For our example, $F(x,y)=x^3-(y^3+y)=0$; treat $y$ as a function of $x$ and take the derivative on both sides
		$$3x^2 - 3y(x)^2y'(x)-y'(x)=0\Rightarrow y'(x) = \frac{3x^2}{3y^2+1}$$
		The above gradient is \textcolor{red}{exact}.
	\end{itemize}
	\begin{center}
		\textbf{Can we apply the same idea to inverse modeling?}
	\end{center}
	
\end{frame}


\begin{frame}
	\frametitle{Example}
	
	\begin{itemize}
		%		\item A simple approach is to save part or all intermediate steps, and ``back-propagate''. This approach is expensive in both computation and memory\footnote{Ablin, Pierre, Gabriel Peyré, and Thomas Moreau. ``Super-efficiency of automatic differentiation for functions defined as a minimum.''}.
		%		\item Nevertheless, the simple approach works in some scenarios where accuracy or cost is not an issue, e.g., automatic differetiation of soft-DTW and Sinkhorn distance. 
		\item An efficient way is to apply the \textcolor{red}{implicit function theorem}. For our example, $F(x,y)=x^3-(y^3+y)=0$, treat $y$ as a function of $x$ and take the derivative on both sides
		$$3x^2 - 3y(x)^2y'(x)-1=0\Rightarrow y'(x) = \frac{3x^2-1}{3y(x)^2}$$
		The above gradient is \textcolor{red}{exact}.
	\end{itemize}
	\begin{center}
		\textbf{Can we apply the same idea to inverse modeling?}
	\end{center}
	
\end{frame}


\begin{frame}
	\frametitle{Physics Constrained Learning (PCL)}
	$${\small    \min_{\theta}\; L_h(u_h) \quad \mathrm{s.t.}\;\; F_h(\theta, u_h) = 0}$$
	\begin{itemize}
		\item Assume that we solve for $u_h=G_h(\theta)$ with $F_h(\theta, u_h)=0$, and then
		$${\small\tilde L_h(\theta)  = L_h(G_h(\theta))}$$
		\item Applying the \textcolor{red}{implicit function theorem}
		{  \scriptsize
			\begin{equation*}
				\frac{{\partial {F_h(\theta, u_h)}}}{{\partial \theta }} + {\frac{{\partial {F_h(\theta, u_h)}}}{{\partial {u_h}}}}
				\textcolor{red}{\frac{\partial G_h(\theta)}{\partial \theta}}
				= 0 \Rightarrow
				\textcolor{red}{\frac{\partial G_h(\theta)}{\partial \theta}} =  -\Big( \frac{{\partial {F_h(\theta, u_h)}}}{{\partial {u_h}}} \Big)^{ - 1} \frac{{\partial {F_h(\theta, u_h)}}}{{\partial \theta }}
			\end{equation*}
		}
		\item Finally we have
		{\scriptsize
			\begin{equation*}
				\boxed{\frac{{\partial {{\tilde L}_h}(\theta )}}{{\partial \theta }}
					= \frac{\partial {{ L}_h}(u_h )}{\partial u_h}\frac{\partial G_h(\theta)}{\partial \theta}=
					- \textcolor{red}{ \frac{{\partial {L_h}({u_h})}}{{\partial {u_h}}} } \;
					\textcolor{blue}{ \Big( {\frac{{\partial {F_h(\theta, u_h)}}}{{\partial {u_h}}}\Big|_{u_h = {G_h}(\theta )}} \Big)^{ - 1} } \;
					\textcolor{ForestGreen}{ \frac{{\partial {F_h(\theta, u_h)}}}{{\partial \theta }}\Big|_{u_h = {G_h}(\theta )} }
				}
			\end{equation*}
		}
		
	\end{itemize}
	
\end{frame}





\begin{frame}
	\frametitle{Physics Constrained Learning for Stiff Problems}
	
	\begin{itemize}
		\item For stiff problems, better to resolve physics using PCL.
		\item Consider a model problem 
		\begin{gather*}
			\min_{\theta} \|u-u_0\|^2_2 \qquad \text{s.t.} \;\; Au = \theta y
		\end{gather*}
	\vspace{-0.5cm}
	\begin{align*}
		\text{PCL}: &&\ \min_\theta \tilde L_h(\theta) &= \|\theta A^{-1} y - u_0\|^2_2 = (\theta-1)^2\|u_0\|_2^2\\
				\text{Penalty Method}: &&\ 
			\min_{\theta, u_h}\tilde L_h(\theta, u_h) &= \|u_h-u_0\|^2_2 + \lambda \|Au_h -\theta y\|_2^2
	\end{align*}
\begin{theorem}
	The condition number of $\mathbf{A}_\lambda$ is 
	\begin{equation*}
		\liminf_{\lambda\rightarrow \infty}\kappa(\mathbf{A}_\lambda)  \geq  \kappa(A)^2,\qquad \mathbf{A}_\lambda = \begin{bmatrix}
			I & 0\\
			\sqrt{\lambda}A & -\sqrt{\lambda}y
		\end{bmatrix}, \qquad 
		\mathbf{y} = \begin{bmatrix}
			u_0\\ 0
		\end{bmatrix}
	\end{equation*}
	and therefore, the condition number of the unconstrained optimization problem from the penalty method is equal to the the square of the condition number of the PCL asymptotically. 
\end{theorem}
 	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Physics Constrained Learning for Stiff Problems}
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=1.0\textwidth]{figures/pcl.png}
	\end{figure}
\end{frame}



\begin{frame}
	\frametitle{PCL: Backbone of the ADCME Infrastructure}
	
	
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.9\textwidth]{figures/infrastructure.png}
	\end{figure}
\end{frame}


\section{Distributed Computing via MPI}
\begin{frame}
	\frametitle{Parallel Computing}
	\begin{itemize}
		\item Parallel computing is essential for accelerating simulation and satisfying demanding memory requirements.
	\end{itemize}
	\begin{figure}[hbt]
		\includegraphics[width=0.8\textwidth]{figures/parallel}
	\end{figure}
\end{frame}


\begin{frame}
	\frametitle{Distributed Optimization}
	\begin{itemize}
		\item ADCME also supports MPI-based distributed computing. The parallel model is designed specially for scientific computing. 

	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.48\textwidth]{figures/distributed1}~
		\includegraphics[width=0.48\textwidth]{figures/distributed2}
	\end{figure}

\item Key idea: \textcolor{red}{Everything is an operator}. Computation and communications are converters of data streams (tensors) through the computational graph. 

\begin{center}
\texttt{mpi\_bcast}, \texttt{mpi\_sum}, \texttt{mpi\_send}, \texttt{mpi\_recv}, \texttt{mpi\_halo\_exchange}, ...
\end{center}

	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Parallel Computing Model for a Single Processor}
	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/adcme_parallel.png}
	\end{figure}
\end{frame}


\begin{frame}
	\frametitle{Mismatched MPI Calls in Hybrid Parallel Computing}
	\begin{itemize}
		\item Without any additional synchronization mechanisms, we may encounter mismatched MPI calls. 
	\end{itemize}
	\begin{figure}[hbt]
		\includegraphics[width=0.6\textwidth]{figures/adcme_parallel3.png}
	\end{figure}
\end{frame}



\begin{frame}
	\frametitle{Hybrid Parallel Computing}
	\begin{figure}
	\item We use \textbf{dependency injection} techniques to ensure consistency.
	\end{figure}
	\begin{figure}[hbt]
		\includegraphics[width=0.6\textwidth]{figures/adcme_parallel2.png}
	\end{figure}
\end{frame}






\begin{frame}
	\frametitle{Interoperability with Hypre}

	\begin{equation*}
		\begin{aligned}
			\nabla \cdot (\textcolor{red}{\textsf{NN}_\theta(\bx)} \nabla u(\bx)) & = f(\bx) & \bx \in \Omega \\ 
			u(\bx) &= 0 & \bx \in \partial \Omega
		\end{aligned}
	\end{equation*}
The discretization leads to a linear system, which is solved using Hypre.
		
\begin{figure}[hbt]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/mpiscalability}
\end{figure}
	

\end{frame}





\section{Code Example}


\begin{frame}
	\frametitle{Inverse Modeling of the Stokes Equation}
	
	\begin{itemize}
		\item The governing equation for the Stokes problem
		$$\begin{aligned} -\textcolor{red}{\nu(\mathbf{x})}\Delta \mathbf{u} + \nabla p &= \mathbf{f} & \text{ in } \Omega \\ \nabla \cdot \mathbf{u} &= 0 & \text{ in } \Omega \\ \mathbf{u} &= \mathbf{0} & \text{ on } \partial \Omega \end{aligned}$$
	\end{itemize}
	
	
	\begin{minipage}[c]{0.7\textwidth}
		\begin{itemize}
			\item The weak form is given by 
			\begin{equation*}
			\begin{aligned}
			(\textcolor{red}{\nu(\mathbf{x})} \nabla u, \nabla v) &- (p, \nabla \cdot v) &&= (f, v)\\
			(\nabla \cdot u, q) & &&= 0
			\end{aligned}
			\end{equation*}
		\end{itemize}
	\end{minipage}~
	\begin{minipage}[c]{0.29\textwidth}
		\includegraphics[width=1.0\textwidth]{figures/stokesuvp}
	\end{minipage}
	
	
\end{frame}


\begin{frame}[fragile]{}
	\frametitle{Inverse Modeling of the Stokes Equation}
	
	\begin{minted}[escapeinside=||,fontsize=\small]{julia}
xy = gauss_nodes(m, n, h)
|\textbf{\colorbox{green}{nu = \texttt{squeeze(fc(xy, [20,20,20,1]))}}}|
K = nu*constant(|\underline{\texttt{compute\_fem\_laplace\_matrix}}|(m, n, h))
B = constant(|\underline{\texttt{compute\_interaction\_matrix}}|(m, n, h))
Z = [K -B'
-B spdiag(zeros(size(B,1)))]

# Impose boundary conditions
bd = bcnode("all", m, n, h)
bd = [bd; bd .+ (m+1)*(n+1); ((1:m) .+ 2(m+1)*(n+1))]
Z, _ = |\underline{\texttt{fem\_impose\_Dirichlet\_boundary\_condition1}}|(Z, bd, m, n, h)

# Calculate the source term 
F1 = |\underline{\texttt{eval\_f\_on\_gauss\_pts}}|(f1func, m, n, h)
F2 = |\underline{\texttt{eval\_f\_on\_gauss\_pts}}|(f2func, m, n, h)
F = |\underline{\texttt{compute\_fem\_source\_term}}|(F1, F2, m, n, h)
rhs = [F;zeros(m*n)]
rhs[bd] .= 0.0

sol = Z\rhs 
	\end{minted}
	
	
\end{frame}



\begin{frame}[fragile]{}
	\frametitle{Inverse Modeling of the Stokes Equation}
	
	\begin{itemize}
		\item The distinguished feature compared to traditional forward simulation programs: \textcolor{red}{the model output is differentiable with respect to model parameters}!
		\begin{minted}[escapeinside=||,fontsize=\small]{julia}
	loss = sum((sol[idx] - observation[idx])^2)
	g = gradients(loss, get\_collection())
		\end{minted}
		\item Optimization with a one-liner:
		\begin{minted}[escapeinside=||,fontsize=\small]{julia}
	BFGS!(sess, loss)
		\end{minted}	 
	\end{itemize}
	
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/lego}
	\end{figure}
	
\end{frame}





%\begin{frame}
%	\frametitle{Code Example}
%	\begin{itemize}
%		\item  Find $b$ such that $u(0.5)=1.0$ and
%		$$-bu''(x)+u(x) = 8 + 4x - 4x^2, x\in[0,1], u(0)=u(1)=0$$
%	\end{itemize}
%	\begin{figure}[hbt]
%  \includegraphics[width=0.8\textwidth]{figures/code.png}
%\end{figure}
%\end{frame}




\section{Applications}


\begin{frame}
	\frametitle{Constitutive Modeling}
	
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=1.0\textwidth]{figures/ad_constitutive_modeling}
	\end{figure}
\end{frame}


\begin{frame}
	\frametitle{Governing Equations}
	
	\begin{equation}\label{equ:momentum}
		\begin{aligned}
			\underbrace{\sigma_{ij,j}}_{\mbox{stress}} + \rho \underbrace{b_i}_{\mbox{external force}} &= \rho \underbrace{\ddot u_i}_{\mbox{velocity}}\\
			\underbrace{\varepsilon_{ij}}_{\mbox{strain}} &= \frac{1}{2}(u_{j,i}+u_{i,j})
		\end{aligned}
	\end{equation}
	
	
	\begin{itemize}
		\item \textbf{Observable}: external/body force $b_i$, displacements $u_i$ (strains $\varepsilon_{ij}$ can be computed from $u_i$); density $\rho$ is known.  
		\item \textbf{Unobservable}: stress $\sigma_{ij}$. 
		\item Data-driven Constitutive Relations: modeling the strain-stress relation using a neural network
		\begin{equation}\label{equ:nn}
			\boxed{\mbox{stress} =\mathcal{M}_{\theta}(\mbox{strain},\ldots)}
		\end{equation}
		and the neural network is trained by coupling Eq.~\ref{equ:momentum} and Eq.~\ref{equ:nn}.
	\end{itemize}
	
	
\end{frame}

\begin{frame}
	\frametitle{Residual Learning using Full-field Data}
	\begin{itemize}
		\item Weak form of balance equations of linear momentum 
		{\footnotesize
			\begin{align*}
				P_i(\theta) &= \int_V \rho \ddot u_i \delta u_i dVt + \int_{V} \textcolor{blue}{\underbrace{\textcolor{blue}{\sigma_{ij}(\theta)}}_{\mathclap{\textcolor{blue}{\mbox{embedded neural network}}}}} \delta \varepsilon_{ij}dV\\
				F_i &= \int_{V}\rho b_i \delta u_i dV + \int_{\partial V} t_i\delta u_idS
			\end{align*}
		}
		\item Train the neural network by 
		{\scriptsize $$\boxed{L(\theta) = \min_{\theta}\;\sum_{i=1}^N(P_i(\theta) - F_i)^2}$$}
		The gradient $\nabla L(\theta)$ is computed via automatic differentiation.
	\end{itemize}
	\begin{figure}[hbt]
		\includegraphics[width=0.75\textwidth]{figures/rnn}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Representation of Constitutive Relations}
	\begin{itemize}
		\item Proper form of constitutive relation is crucial for numerical stability
		{\footnotesize\begin{align*}
				\mbox{Elasticity} &\Rightarrow \bm\sigma = \mathsf{C}_{\theta}\bm\epsilon \\
				\mbox{Hyperelasticity } &\Rightarrow \begin{cases}\bm\sigma =\mathcal{M}_{\theta}(\bm\epsilon) & \mbox{(Static)} \\
					\bm{\sigma}^{n+1}  =  \ChoL_{\bt}(\bm\epsilon^{n+1}) \ChoL_{\bt}(\bm\epsilon^{n+1})^T (\bm{\epsilon}^{n+1} - \bm{\epsilon}^{n})  + \bm{\sigma}^{n}  & \mbox{(Dynamic)} \end{cases} \\
				\mbox{Elaso-Plasticity} &\Rightarrow \bm\sigma^{n+1} = \ChoL_{\bt}(\bm\epsilon^{n+1},\bm{\epsilon}^{n},\bm{\sigma}^{n}) \ChoL_{\bt}(\bm\epsilon^{n+1},\bm{\epsilon}^{n},\bm{\sigma}^{n})^T (\bm{\epsilon}^{n+1} - \bm{\epsilon}^{n})  + \bm{\sigma}^{n} 
			\end{align*}
		}{\footnotesize$$\ChoL_{\bt} = \begin{bmatrix}
				L_{1111}  &  & &  &       &\\
				L_{2211}  & L_{2222} & &   & &\\
				L_{3311}  &  L_{3322}               & L_{3333} &  & &\\
				&                 &                 & L_{2323}&  &\\
				&               &                  &                & L_{1313} &\\
				&                 &                  &                &                 &L_{1212}\\
			\end{bmatrix}$$}
		\item \textcolor{red}{Weak convexity}: $\ChoL_{\bt}\ChoL_{\bt}^T \succ 0$
		\item \textcolor{red}{Time consistency}:  $\bm\sigma^{n+1} \rightarrow \bm \sigma^n$ when $\bm\epsilon^{n+1} \rightarrow \bm \epsilon^n$
	\end{itemize}
	
\end{frame}


\begin{frame}
	\frametitle{Modeling Elasto-plasticity}
	\begin{itemize}
		\item Comparison of different neural network architectures 
		\begin{align*}
			\bm\sigma^{n+1} &= \ChoL_{\bt}(\bm\epsilon^{n+1},\bm{\epsilon}^{n},\bm{\sigma}^{n}) \ChoL_{\bt}(\bm\epsilon^{n+1},\bm{\epsilon}^{n},\bm{\sigma}^{n})^T (\bm{\epsilon}^{n+1} - \bm{\epsilon}^{n})  + \bm{\sigma}^{n} \\
			\bm{\sigma}^{n+1} &=  \mathsf{NN}_{\bt}(\bm\epsilon^{n+1},\bm{\epsilon}^{n},\bm{\sigma}^{n})\\
			\bm{\sigma}^{n+1} &=  \mathsf{NN}_{\bt}(\bm\epsilon^{n+1},\bm{\epsilon}^{n},\bm{\sigma}^{n}) + \bm{\sigma}^{n}
		\end{align*}
	\end{itemize}
	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/nncons}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Modeling Elasto-plasticity: Multi-scale}
	\begin{figure}[hbt]
		\includegraphics[width=0.8\textwidth]{figures/plasticity}
	\end{figure}
\end{frame}


\begin{frame}
	\frametitle{Static Hyperelasticity Problem}
	
	\begin{itemize}
		\item Consider an axisymmetric Mooney-Rivlin hyperelastic incompressible material with an energy density function 
		\begin{align*}
			W(\lambda_1, \lambda_2, \lambda_3) &= \mu(\lambda_1^2+\lambda_2^2+\lambda_3^2-3) + \alpha (\lambda_1^2\lambda_2^2 + \lambda_2^2\lambda_3^2+\lambda_3^2\lambda_1^2 - 3)\\
			J &= \lambda_1\lambda_2\lambda_3 = 1 
		\end{align*}
		
		\item The constitutive relations is modeled as 
		$$\mathcal{N}_\theta: (\lambda_1, \lambda_2) \rightarrow (P_1, P_2)$$
		Here $(P_1, P_2)$ is the stress tensor. 
		
	\end{itemize}
	\begin{figure}[hbt]
		\includegraphics[width=0.6\textwidth]{figures/static_rubber}
	\end{figure}
\end{frame}



\begin{frame}
	\frametitle{Comparison with Traditional Basis Functions}
	\begin{figure}[hbt]
		\includegraphics[width=0.8\textwidth]{figures/vs}
	\end{figure}
\end{frame}



\newcommand{\bsigma}[0]{\bm{\sigma}}
\newcommand{\bepsilon}[0]{\bm{\epsilon}}



\begin{frame}
	\frametitle{Learning Spatially-varying fields}
	
	\begin{itemize}
		\item Hyperelasticity: minimizing the neo-Hookean stored energy 
		$$\min_u \psi = \frac{\mu}{2}(I_c - 2) - \frac{\mu}{2}\log(J) + \frac{\lambda}{8}\log(J)^2$$
		where 
		$$F = I + \nabla u, \ C = F^TF, \ J = \text{det}(C), \ I_c = \text{trace}(C)$$
		\item Lam\'e parameters
		$$\lambda = \frac{\textcolor{red}{E}\nu}{(1+\nu)(1-2\nu)}, \quad \mu = \frac{\textcolor{red}{E}}{2(1+\nu)}$$
	\end{itemize}
	\begin{figure}[hbt]	
		\centering
		\includegraphics[width=0.8\textwidth]{figures/spatially_varying}
	\end{figure}
\end{frame}


\begin{frame}
	\frametitle{Learning Spatially-varying fields}
	\begin{itemize}
		\item DNN provides expressive data-driven models and regularization (e.g., spatial dependencies). 
	\end{itemize}
	\vspace{-0.1cm}
	\begin{figure}[hbt]	
		\centering
		\includegraphics[width=1.0\textwidth]{figures/Hyperelasticity}
	\end{figure}
\end{frame}



\begin{frame}
	
	\frametitle{Poroelasticity}
	%	
	\begin{itemize}
		\item Multi-physics Interaction of Coupled Geomechanics and Multi-Phase Flow Equations 
		{\small
			\begin{align*}
				\mathrm{div}\bsigma(\bu) - b \nabla p &= 0\\
				\frac{1}{M} \frac{\partial p}{\partial t} + b\frac{\partial \epsilon_v(\bu)}{\partial t} - \nabla\cdot\left(\frac{k}{B_f\mu}\nabla p\right) &= f(x,t)	\\
				\bsigma &= \bsigma(\bepsilon, \dot\bepsilon)
			\end{align*}
		}
		\item Approximate the constitutive relation by a neural network
		{\small
			$$\bsigma^{n+1} = \mathcal{NN}_{\bt} (\bsigma^n, \bepsilon^n) + H\bepsilon^{n+1}$$}
	\end{itemize}		
	\begin{figure}[hbt]	
		\centering
		\includegraphics[width=0.5\textwidth]{figures/ip}~
		\includegraphics[width=0.3\textwidth]{figures/cell}
	\end{figure}
	
\end{frame}



\begin{frame}
	\frametitle{Neural Networks: Inverse Modeling of Viscoelasticity}
	
	\begin{itemize}
		\item We propose the following form for modeling viscosity (assume the time step size is fixed):
		%   $$\bsigma^{n+1} = \mathcal{NN}_{\bt} (\bsigma^n, \bepsilon^n) + H\bepsilon^{n+1}$$
		$$\bsigma^{n+1} - \bsigma^{n} = \mathcal{NN}_{\bt} (\bsigma^n, \bepsilon^n) + H (\bepsilon^{n+1} - \bepsilon^n)$$
	\end{itemize}
	\begin{itemize}
		\item $H$ is a free optimizable \textcolor{red}{symmetric positive definite matrix} (SPD). Hence the numerical stiffness matrix is SPD.
		\item Implicit linear equation
		%   $$\bsigma^{n+1} = H(\bepsilon^{n+1} - \bepsilon^n) +  \left(\mathcal{NN}_{\bt} (\bsigma^n, \bepsilon^n)+H\bepsilon^n \right)$$
		$$\bsigma^{n+1} - H \bepsilon^{n+1} = - H \bepsilon^n
		+ \mathcal{NN}_{\bt} (\bsigma^n, \bepsilon^n) + \bsigma^{n}:= \mathcal{NN}_{\bt}^* (\bsigma^n, \bepsilon^n)$$
		\item Linear system to solve in each time step $\Rightarrow$ good balance between \textcolor{red}{numerical stability} and \textcolor{red}{computational cost}.
		\item Good performance in our numerical examples.
	\end{itemize}
\end{frame}

% \begin{frame}
% 	\frametitle{Neural Networks: Inverse Modeling of Viscoelasticity}
% 	\begin{figure}
% 		\centering
% 		\includegraphics[width=0.7\textwidth]{figures/strainstreess}
% 	\end{figure}
% \end{frame}

\begin{frame}
	\frametitle{Training Strategy and Numerical Stability}
	
	\begin{itemize}
		\item Physics constrained learning = improved numerical stability in predictive modeling.
		\item For simplicity, consider two strategies to train an NN-based constitutive relation using direct data $\{(\epsilon_o^n, \sigma_o^n)\}_n$
		$$\Delta \sigma^n = H \Delta \epsilon^n + \mathcal{NN}_{\bt} (\sigma^n, \epsilon^n),\quad H \succ 0$$
		\item Training with input-output pairs
		$$\min_{\bt} \sum_n \Big(\sigma_o^{n+1} - \big(H\epsilon_o^{n+1} +  \mathcal{NN}_{\bt}^* (\sigma_o^n, \epsilon_o^n)\big) \Big)^2$$
		\item Better stability using training on trajectory = \textcolor{red}{physics constrained learning}
		\begin{gather*}
			\min_{\bt} \ \sum_n (\sigma^n(\bt) - \sigma_o^n)^2 \\
			\text{s.t. }\text{I.C.} \ \sigma^1 = \sigma^1_o \text{ and time integrator\ }
			{\small \Delta \sigma^n = H \Delta \epsilon^n + \mathcal{NN}_{\bt} (\sigma^n, \epsilon^n)}
		\end{gather*}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Experimental Data}
	
	\includegraphics[width=1.0\textwidth]{figures/experiment}
	
	{\scriptsize Experimental data from: Javidan, Mohammad Mahdi, and Jinkoo Kim. ``Experimental and numerical Sensitivity Assessment of Viscoelasticity for polymer composite Materials.'' Scientific Reports 10.1 (2020): 1--9.}
	
\end{frame}




\begin{frame}
	\frametitle{Poroelasticity}
	
	\begin{itemize}
		\item Comparison with space varying linear elasticity approximation
		\begin{equation*}
			\bsigma = H(x, y) \bepsilon
		\end{equation*}
	\end{itemize}
	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/visco1}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Poroelasticity}
	\begin{figure}[hbt]
		\includegraphics[width=0.7\textwidth]{figures/visco2}
	\end{figure}
	
\end{frame}




\begin{frame}
	\frametitle{Navier-Stokes Equation}
	
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.49\textwidth]{figures/advertisement}~
		\includegraphics[width=0.49\textwidth]{figures/computational_graph}
	\end{figure}
\end{frame}



\begin{frame}
	\frametitle{Navier-Stokes Equation}
	\begin{itemize}
		\item Data: $(u, v)$
		\item Unknown: $\nu(\mathbf{x})$ (represented by a deep neural network)
		\item Prediction: $p$ (absent in the training data) 
		\item The DNN provides regularization, which generalizes the estimation better!
	\end{itemize}
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=1.0\textwidth]{figures/ns_result}~
	\end{figure}
\end{frame}




\begin{frame}
	\frametitle{ADSeismic.jl: A General Approach to Seismic Inversion}
	\begin{itemize}
		\item Many seismic inversion problems can be solved within a unified framework. 
	\end{itemize}
	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/adseimic.jpeg}
	\end{figure}
	
\end{frame}


\begin{frame}
	\frametitle{NNFWI: Neural-network-based Full-Waveform Inversion}
	
	\begin{itemize}
		\item Estimate velocity models from seismic observations. 
	\end{itemize}
	\begin{minipage}[t]{0.3\textwidth}
		\begin{equation*}
			\qquad\frac{\partial^2 u}{\partial t^2} = \nabla \cdot (\textcolor{red}{m}^2 \nabla u) + f
		\end{equation*}
	\end{minipage}~
	\begin{minipage}[t]{0.69\textwidth}
		\begin{figure}[hbt]
			\centering
			\includegraphics[width=0.7\textwidth]{figures/bp}
		\end{figure}
	\end{minipage}
	
	
	
	\begin{figure}[hbt]
		\includegraphics[width=0.6\textwidth]{figures/fwi}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{NNFWI: Neural-network-based Full-Waveform Inversion}
	
	\begin{itemize}
		\item Inversion results with a noise level $\sigma = \sigma_0$
		\begin{figure}[hbt]
			\centering
			\includegraphics[width=0.8\textwidth]{figures/bp_results}
		\end{figure}
		
		\item Inversion results for the same loss function value:
		\begin{figure}[hbt]
			\centering
			\includegraphics[width=0.8\textwidth]{figures/bp_results_2}
		\end{figure}
		
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{ADSeismic.jl: Performance Benchmark}
	\begin{itemize}
		\item Performance is a key focus of ADCME.  
		\item ADCME enables us to utilize heterogeneous (CPUs, GPUs, and TPUs) and distributed (CPU clusters) computing environments.
		
		{\small Fortran: {open-source Fortran90 programs SEISMIC\_CPML}}
	\end{itemize}
	\begin{figure}[hbt]
		\includegraphics[width=0.6\textwidth]{figures/adsesimic_mpi}
	\end{figure}
\end{frame}




\begin{frame}
	\frametitle{A General Approach to Inverse Modeling}
	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/summary.png}
	\end{figure}
\end{frame}


%}
%\usebackgroundtemplate{}
%----------------------------------------------------------------------------------------
%    PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

%------------------------------------------------



\end{document} 