%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%    PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass[usenames,dvipsnames]{beamer}
\usepackage{animate}
\usepackage{float}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{extarrows}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{minted}

\newcommand{\ChoL}{\mathsf{L}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\ii}{\mathrm{i}}
\newcommand{\bxi}{\bm{\xi}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bM}{\mathbf{M}}

\newcommand{\by}{\mathbf{y}}
\newcommand{\bw}{\mathbf{w}}

\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\bt}[0]{\bm{\theta}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bzero}{\mathbf{0}}
\renewcommand{\bf}{\mathbf{f}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}[0]{\mathbf{v}}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}


% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{soul}
\newcommand{\red}[1]{\textcolor{red}{#1}}
%
%\usepackage{graphicx} % Allows including images
%\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
%
%
%\usepackage{amsthm}
%
%\usepackage{todonotes}
%\usepackage{floatrow}
%
%\usepackage{pgfplots,algorithmic,algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
%\usepackage[toc,page]{appendix}
%\usepackage{float}
%\usepackage{booktabs}
%\usepackage{bm}
%
%\theoremstyle{definition}
%
\newcommand{\RR}[0]{\mathbb{R}}
%
%\newcommand{\bx}{\mathbf{x}}
%\newcommand{\ii}{\mathrm{i}}
%\newcommand{\bxi}{\bm{\xi}}
%\newcommand{\bmu}{\bm{\mu}}
%\newcommand{\bb}{\mathbf{b}}
%\newcommand{\bA}{\mathbf{A}}
%\newcommand{\bJ}{\mathbf{J}}
%\newcommand{\bB}{\mathbf{B}}
%\newcommand{\bM}{\mathbf{M}}
%\newcommand{\bF}{\mathbf{F}}
%
%\newcommand{\by}{\mathbf{y}}
%\newcommand{\bw}{\mathbf{w}}
%\newcommand{\bn}{\mathbf{n}}
%
%\newcommand{\bX}{\mathbf{X}}
%\newcommand{\bY}{\mathbf{Y}}
%\newcommand{\bs}{\mathbf{s}}
%\newcommand{\sign}{\mathrm{sign}}
%\newcommand{\bt}[0]{\bm{\theta}}
%\newcommand{\bc}{\mathbf{c}}
%\newcommand{\bzero}{\mathbf{0}}
%\renewcommand{\bf}{\mathbf{f}}
%\newcommand{\bu}{\mathbf{u}}
%\newcommand{\bv}[0]{\mathbf{v}}

\AtBeginSection[]
{
   \begin{frame}
       \frametitle{Outline}
       \tableofcontents[currentsection]
   \end{frame}
}

%----------------------------------------------------------------------------------------
%    TITLE PAGE
%----------------------------------------------------------------------------------------
\usepackage{bm}
\newcommand*{\TakeFourierOrnament}[1]{{%
\fontencoding{U}\fontfamily{futs}\selectfont\char#1}}
\newcommand*{\danger}{\TakeFourierOrnament{66}}

\title[ML for Computational Engineering]{Machine Learning for Inverse Problems in Computational Engineering} % The short title appears at the bottom of every slide, the full title is only on the title page

\author[ADCME]{Kailai Xu and Eric Darve \\ \url{https://github.com/kailaix/ADCME.jl}} % Your name
%\institute[] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
%{
%%ICME, Stanford University \\ % Your institution for the title page
%%\medskip
%%\textit{kailaix@stanford.edu}\quad \textit{darve@stanford.edu} % Your email address
%}
\date{}% Date, can be changed to a custom date
% Mathematics of PDEs


\begin{document}

\usebackgroundtemplate{%
\begin{picture}(0,250)
\centering
	{{\includegraphics[width=1.0\paperwidth]{figures/background}}}
\end{picture}
  } 
%\usebackgroundtemplate{%
%  \includegraphics[width=\paperwidth,height=\paperheight]{figures/back}} 
\begin{frame}

\titlepage % Print the title page as the first slide

%dfa
\end{frame}
\usebackgroundtemplate{}

\section{Inverse Modeling}




\begin{frame}
	\frametitle{Inverse Modeling}
	\begin{figure}
		\centering
		\includegraphics[width=1.0\textwidth]{figures/inverse3}
	\end{figure}
\end{frame}


\begin{frame}
	\frametitle{Inverse Modeling}
	\begin{itemize}
		\item \textbf{Inverse modeling} identifies a certain set of parameters or functions with which the outputs of the forward analysis matches the desired result or measurement.
		\item Many real life engineering problems can be formulated as inverse modeling problems: shape optimization for improving the performance of structures, optimal control of fluid dynamic systems, etc.
	\end{itemize}
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/inverse2}
	\end{figure}
\end{frame}




\begin{frame}
	\frametitle{Inverse Modeling}
	We can formulate inverse modeling as a PDE-constrained optimization problem 
	\begin{equation*}
		\min_{\theta} L_h(u_h) \quad \mathrm{s.t.}\; F_h(\theta, u_h) = 0
	\end{equation*}
	\begin{itemize}
		\item The \textcolor{red}{loss function} $L_h$ measures the discrepancy between the prediction $u_h$ and the observation $u_{\mathrm{obs}}$, e.g., $L_h(u_h) = \|u_h - u_{\mathrm{obs}}\|_2^2$. 
		\item $\theta$ is the \textcolor{red}{model parameter} to be calibrated. 
		\item The \textcolor{red}{physics constraints} $F_h(\theta, u_h)=0$ are described by a system of partial differential equations or differential algebraic equations (DAEs); e.g., 
		$$F_h(\theta, u_h) = \mathbf{A}(\theta) u_h - f_h = 0$$
	\end{itemize}
\end{frame}




\begin{frame}
	\frametitle{Function Inverse Problem}
	
	\begin{equation*}
		\min_{\textcolor{red}{f}} L_h(u_h) \quad \mathrm{s.t.}\; F_h(\textcolor{red}{f}, u_h) = 0
	\end{equation*}
	
	What if the unknown is a \textcolor{red}{function} instead of a set of parameters?
\begin{itemize}
	\item Koopman operator in dynamical systems.
	\item Constitutive relations in solid mechanics. 
	\item Turbulent closure relations in fluid mechanics.
	\item ...
\end{itemize}

The candidate solution space is \textcolor{red}{infinite dimensional}.

\end{frame}


\begin{frame}
	\frametitle{Penalty Methods}
	
	\begin{itemize}
		\item Parametrize $f$ with $f_\theta$ and incorporate the physical constraint as a \textcolor{red}{penalty term} (regularization, prior, \ldots) in the loss function.
		\begin{equation*}
	\min_{\theta,\,u_h} L_h(u_h) + \lambda\|F_h(f_\theta, u_h)\|_2^2
		\end{equation*}
		\begin{itemize}
			\item May not satisfy physical constraint $F_h(f_\theta, u_h)=0$ accurately;
			\item Slow convergence for \textcolor{red}{stiff} problems;
			\begin{figure}[hbt]
				\includegraphics[width=0.6\textwidth]{figures/slowconvergence.png}
			\end{figure}
			
			
			\item High dimensional optimization problem; both $\theta$ and $u_h$ are variables.
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Machine Learning for Computational Engineering}
	$$\min_{\theta} L_h(u_h) \quad \mathrm{s.t.}\;\boxed{F_h(\textcolor{red}{NN_\theta}, u_h) = 0} \leftarrow \mbox{Solved numerically}$$
	\vspace{-0.5cm}
	\begin{itemize}
		\item Deep neural networks exhibit capability of approximating high dimensional and complicated functions. 
		\item \textbf{Machine Learning for Computational Engineering}: \textcolor{red}{the unknown function is approximated by a deep neural network, and the physical constraints are enforced by numerical schemes}.
		\item \textcolor{red}{Satisfy the physics to the largest extent}.
	\end{itemize}
	\begin{figure}[hbt]
  \includegraphics[width=0.75\textwidth]{figures/physics_based_machine_learning.png}
\end{figure}
\end{frame}



\begin{frame}
	\frametitle{Gradient Based Optimization}
	\begin{equation}\label{equ:opt}
		\min_{\theta} L_h(u_h) \quad \mathrm{s.t.}\; F_h(\theta, u_h) = 0
		\end{equation}
	
	\begin{itemize}
		\item We can now apply a gradient-based optimization method to (\ref{equ:opt}).
		\item The key is to \textcolor{red}{calculate a descent direction} $g^k$
		$$\theta^{k+1} \gets \theta^k - \alpha g^k$$ 
	\end{itemize}
	
	\begin{figure}[hbt]
	\centering
  \includegraphics[width=0.6\textwidth]{figures/im.pdf}
\end{figure}

\end{frame}



\section{Automatic Differentiation}


\begin{frame}
	\frametitle{Automatic Differentiation}
	The fact that bridges the \textcolor{red}{technical} gap between machine learning and inverse modeling:
	\begin{itemize}
		\item Deep learning (and many other machine learning techniques) and numerical schemes share the same computational model: composition of individual operators. 
	\end{itemize}
	
	
	\begin{minipage}[t]{0.4\textwidth}
		
		\
		
		
		
		\begin{center}
			\textcolor{red}{Mathematical Fact}
			
			\
			
			Back-propagation 
			
			$||$
			
			Reverse-mode
			
			Automatic Differentiation 
			
			$||$
			
			Discrete 
			
			Adjoint-State Method
		\end{center}
	\end{minipage}~
	\begin{minipage}[t]{0.6\textwidth}
		\begin{figure}[hbt]
			\includegraphics[width=0.8\textwidth]{figures/compare-NN-PDE.png}
		\end{figure}
	\end{minipage}
	
\end{frame}

\begin{frame}
	\frametitle{Computational Graph for Numerical Schemes}
	
	\begin{itemize}
		\item To leverage automatic differentiation for inverse modeling, we need to express the numerical schemes in the ``AD language'': computational graph. 
		\item No matter how complicated a numerical scheme is, it can be decomposed into a collection of operators that are interlinked via state variable dependencies. 
	\end{itemize}
	
	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/cgnum}
	\end{figure}
	
	
	
\end{frame}


\begin{frame}
	\frametitle{ADCME: Computational-Graph-based Numerical Simulation}
	
	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/custom}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{How ADCME works}
	\begin{itemize}
		\item ADCME translates your numerical simulation codes to computational graph and then the computations are delegated to a heterogeneous task-based parallel computing environment through TensorFlow runtime. 
	\end{itemize}
	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/routine2.png}
	\end{figure}
\end{frame}



\section{Physics Constrained Learning}

\begin{frame}
	
	
	\frametitle{Challenges in AD}
	
	
	\begin{minipage}{0.695\textwidth}
		\vspace{-6cm}
		\begin{itemize}
			\item Most AD frameworks only deal with \textcolor{red}{explicit operators}, i.e., the functions that has analytical derivatives, or composition of these functions. 
			\item Many scientific computing algorithms are \textcolor{red}{iterative} or \textcolor{red}{implicit} in nature.
			\vspace{1cm}
			{\small
			\begin{table}[]
				\begin{tabular}{@{}lll@{}}
					\toprule
					Linear/Nonlinear & Explicit/Implicit & Expression   \\ \midrule
					Linear           & Explicit          & $y=Ax$       \\
					Nonlinear        & Explicit          & $y = F(x)$   \\
					\textbf{Linear}           & \textbf{Implicit}          & $Ay = x$     \\
					\textbf{Nonlinear}        & \textbf{Implicit}          & $F(x,y) = 0$ \\ \bottomrule
				\end{tabular}
			\end{table}
		}
		\end{itemize}
	\end{minipage}~
	\begin{minipage}[t]{0.3\textwidth}
		\includegraphics[width=1.0\textwidth]{figures/implicitvsexplicit.png}
	\end{minipage}
	
	% Please add the following required packages to your document preamble:
	% \usepackage{booktabs}
	
\end{frame}

\begin{frame}
	\frametitle{Example}
	
	\begin{itemize}
		\item Consider a function $f:x\rightarrow y$, which is implicitly defined by 
		$$F(x,y) = x^3 - (y^3+y) = 0$$
		If not using the cubic formula for finding the roots, the forward computation consists of iterative algorithms, such as the Newton's method and bisection method
	\end{itemize}
	
	
	
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\begin{algorithmic}
			\State $y^0 \gets 0$
			\State $k \gets 0$
			\While {$|F(x, y^k)|>\epsilon$}
			\State $\delta^k \gets F(x, y^k)/F'_y(x,y^k)$
			\State $y^{k+1}\gets y^k - \delta^k$
			\State $k \gets k+1$
			\EndWhile
			\State \textbf{Return} $y^k$
		\end{algorithmic}
	\end{minipage}~
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\begin{algorithmic}
			\State $l \gets -M$, $r\gets M$, $m\gets 0$
			\While {$|F(x, m)|>\epsilon$}
			\State $c \gets \frac{a+b}{2}$
			\If{$F(x, m)>0$}
			\State $a\gets m$
			\Else
			\State $b\gets m$
			\EndIf
			\EndWhile
			\State \textbf{Return} $c$
		\end{algorithmic}
		
	\end{minipage}	
	
\end{frame}

\begin{frame}
	\frametitle{Example}
	
	\begin{itemize}
		%		\item A simple approach is to save part or all intermediate steps, and ``back-propagate''. This approach is expensive in both computation and memory\footnote{Ablin, Pierre, Gabriel Peyr�, and Thomas Moreau. ``Super-efficiency of automatic differentiation for functions defined as a minimum.''}.
		%		\item Nevertheless, the simple approach works in some scenarios where accuracy or cost is not an issue, e.g., automatic differetiation of soft-DTW and Sinkhorn distance. 
		\item An efficient way to do automatic differentiation is to apply the \textcolor{red}{implicit function theorem}. For our example, $F(x,y)=x^3-(y^3+y)=0$; treat $y$ as a function of $x$ and take the derivative on both sides
		$$3x^2 - 3y(x)^2y'(x)-y'(x)=0\Rightarrow y'(x) = \frac{3x^2}{3y^2+1}$$
		The above gradient is \textcolor{red}{exact}.
	\end{itemize}
	\begin{center}
		\textbf{Can we apply the same idea to inverse modeling?}
	\end{center}
	
\end{frame}



\begin{frame}
	\frametitle{Physics Constrained Learning (PCL)}
	$${\small    \min_{\theta}\; L_h(u_h) \quad \mathrm{s.t.}\;\; F_h(\theta, u_h) = 0}$$
	\begin{itemize}
		\item Assume that we solve for $u_h=G_h(\theta)$ with $F_h(\theta, u_h)=0$, and then
		$${\small\tilde L_h(\theta)  = L_h(G_h(\theta))}$$
		\item Applying the \textcolor{red}{implicit function theorem}
		{  \scriptsize
			\begin{equation*}
				\frac{{\partial {F_h(\theta, u_h)}}}{{\partial \theta }} + {\frac{{\partial {F_h(\theta, u_h)}}}{{\partial {u_h}}}}
				\textcolor{red}{\frac{\partial G_h(\theta)}{\partial \theta}}
				= 0 \Rightarrow
				\textcolor{red}{\frac{\partial G_h(\theta)}{\partial \theta}} =  -\Big( \frac{{\partial {F_h(\theta, u_h)}}}{{\partial {u_h}}} \Big)^{ - 1} \frac{{\partial {F_h(\theta, u_h)}}}{{\partial \theta }}
			\end{equation*}
		}
		\item Finally we have
		{\scriptsize
			\begin{equation*}
				\boxed{\frac{{\partial {{\tilde L}_h}(\theta )}}{{\partial \theta }}
					= \frac{\partial {{ L}_h}(u_h )}{\partial u_h}\frac{\partial G_h(\theta)}{\partial \theta}=
					- \textcolor{red}{ \frac{{\partial {L_h}({u_h})}}{{\partial {u_h}}} } \;
					\textcolor{blue}{ \Big( {\frac{{\partial {F_h(\theta, u_h)}}}{{\partial {u_h}}}\Big|_{u_h = {G_h}(\theta )}} \Big)^{ - 1} } \;
					\textcolor{ForestGreen}{ \frac{{\partial {F_h(\theta, u_h)}}}{{\partial \theta }}\Big|_{u_h = {G_h}(\theta )} }
				}
			\end{equation*}
		}
		
	\end{itemize}
	
\end{frame}





\begin{frame}
	\frametitle{Physics Constrained Learning for Stiff Problems}
	
	\begin{itemize}
		\item For stiff problems, better to resolve physics using PCL.
		\item Consider a model problem 
		\begin{gather*}
			\min_{\theta} \|u-u_0\|^2_2 \qquad \text{s.t.} \;\; Au = \theta y
		\end{gather*}
	\vspace{-0.5cm}
	\begin{align*}
		\text{PCL}: &&\ \min_\theta \tilde L_h(\theta) &= \|\theta A^{-1} y - u_0\|^2_2 = (\theta-1)^2\|u_0\|_2^2\\
				\text{Penalty Method}: &&\ 
			\min_{\theta, u_h}\tilde L_h(\theta, u_h) &= \|u_h-u_0\|^2_2 + \lambda \|Au_h -\theta y\|_2^2
	\end{align*}
\begin{theorem}
	The condition number of $\mathbf{A}_\lambda$ is 
	\begin{equation*}
		\liminf_{\lambda\rightarrow \infty}\kappa(\mathbf{A}_\lambda)  \geq  \kappa(A)^2,\qquad \mathbf{A}_\lambda = \begin{bmatrix}
			I & 0\\
			\sqrt{\lambda}A & -\sqrt{\lambda}y
		\end{bmatrix}, \qquad 
		\mathbf{y} = \begin{bmatrix}
			u_0\\ 0
		\end{bmatrix}
	\end{equation*}
	and therefore, the condition number of the unconstrained optimization problem from the penalty method is equal to the the square of the condition number of the PCL asymptotically. 
\end{theorem}
 	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Physics Constrained Learning for Stiff Problems}
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=1.0\textwidth]{figures/pcl.png}
	\end{figure}
\end{frame}



\begin{frame}
	\frametitle{PCL: Backbone of the ADCME Infrastructure}
	
	
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.9\textwidth]{figures/infrastructure.png}
	\end{figure}
\end{frame}



\begin{frame}
	\frametitle{A General Approach to Inverse Modeling}
	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/summary.png}
	\end{figure}
\end{frame}


%}
%\usebackgroundtemplate{}
%----------------------------------------------------------------------------------------
%    PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

%------------------------------------------------



\end{document} 