%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%    PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass[usenames,dvipsnames]{beamer}
\usepackage{animate}
\usepackage{float}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{extarrows}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{minted}

\newcommand{\ChoL}{\mathsf{L}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\ii}{\mathrm{i}}
\newcommand{\bxi}{\bm{\xi}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bM}{\mathbf{M}}

\newcommand{\by}{\mathbf{y}}
\newcommand{\bw}{\mathbf{w}}

\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\bt}[0]{\bm{\theta}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bzero}{\mathbf{0}}
\renewcommand{\bf}{\mathbf{f}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}[0]{\mathbf{v}}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}


% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{soul}
\newcommand{\red}[1]{\textcolor{red}{#1}}
%
%\usepackage{graphicx} % Allows including images
%\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
%
%
%\usepackage{amsthm}
%
%\usepackage{todonotes}
%\usepackage{floatrow}
%
%\usepackage{pgfplots,algorithmic,algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
%\usepackage[toc,page]{appendix}
%\usepackage{float}
%\usepackage{booktabs}
%\usepackage{bm}
%
%\theoremstyle{definition}
%
\newcommand{\RR}[0]{\mathbb{R}}
%
%\newcommand{\bx}{\mathbf{x}}
%\newcommand{\ii}{\mathrm{i}}
%\newcommand{\bxi}{\bm{\xi}}
%\newcommand{\bmu}{\bm{\mu}}
%\newcommand{\bb}{\mathbf{b}}
%\newcommand{\bA}{\mathbf{A}}
%\newcommand{\bJ}{\mathbf{J}}
%\newcommand{\bB}{\mathbf{B}}
%\newcommand{\bM}{\mathbf{M}}
%\newcommand{\bF}{\mathbf{F}}
%
%\newcommand{\by}{\mathbf{y}}
%\newcommand{\bw}{\mathbf{w}}
%\newcommand{\bn}{\mathbf{n}}
%
%\newcommand{\bX}{\mathbf{X}}
%\newcommand{\bY}{\mathbf{Y}}
%\newcommand{\bs}{\mathbf{s}}
%\newcommand{\sign}{\mathrm{sign}}
%\newcommand{\bt}[0]{\bm{\theta}}
%\newcommand{\bc}{\mathbf{c}}
%\newcommand{\bzero}{\mathbf{0}}
%\renewcommand{\bf}{\mathbf{f}}
%\newcommand{\bu}{\mathbf{u}}
%\newcommand{\bv}[0]{\mathbf{v}}

\AtBeginSection[]
{
   \begin{frame}
       \frametitle{Outline}
       \tableofcontents[currentsection]
   \end{frame}
}

%----------------------------------------------------------------------------------------
%    TITLE PAGE
%----------------------------------------------------------------------------------------
\usepackage{bm}
\newcommand*{\TakeFourierOrnament}[1]{{%
\fontencoding{U}\fontfamily{futs}\selectfont\char#1}}
\newcommand*{\danger}{\TakeFourierOrnament{66}}

\title[ML for Computational Engineering]{Machine Learning for Inverse Problems in Computational Engineering} % The short title appears at the bottom of every slide, the full title is only on the title page

\author[ADCME]{Kailai Xu and Eric Darve \\ \url{https://github.com/kailaix/ADCME.jl}} % Your name
%\institute[] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
%{
%%ICME, Stanford University \\ % Your institution for the title page
%%\medskip
%%\textit{kailaix@stanford.edu}\quad \textit{darve@stanford.edu} % Your email address
%}
\date{}% Date, can be changed to a custom date
% Mathematics of PDEs


\begin{document}

\usebackgroundtemplate{%
\begin{picture}(0,250)
\centering
	{{\includegraphics[width=1.0\paperwidth]{figures/background}}}
\end{picture}
  } 
%\usebackgroundtemplate{%
%  \includegraphics[width=\paperwidth,height=\paperheight]{figures/back}} 
\begin{frame}

\titlepage % Print the title page as the first slide

%dfa
\end{frame}
\usebackgroundtemplate{}

\section{Inverse Modeling}




\begin{frame}
	\frametitle{Inverse Modeling}
	\begin{figure}
		\centering
		\includegraphics[width=1.0\textwidth]{figures/inverse3}
	\end{figure}
\end{frame}






\begin{frame}
	\frametitle{Inverse Modeling}
	We can formulate inverse modeling as a PDE-constrained optimization problem 
	\begin{equation*}
		\min_{\theta} L_h(u_h) \quad \mathrm{s.t.}\; F_h(\theta, u_h) = 0
	\end{equation*}
	\begin{itemize}
		\item The \textcolor{red}{loss function} $L_h$ measures the discrepancy between the prediction $u_h$ and the observation $u_{\mathrm{obs}}$, e.g., $L_h(u_h) = \|u_h - u_{\mathrm{obs}}\|_2^2$. 
		\item $\theta$ is the \textcolor{red}{model parameter} to be calibrated. 
		\item The \textcolor{red}{physics constraints} $F_h(\theta, u_h)=0$ are described by a system of partial differential equations or differential algebraic equations (DAEs); e.g., 
		$$F_h(\theta, u_h) = \mathbf{A}(\theta) u_h - f_h = 0$$
	\end{itemize}
\end{frame}




\begin{frame}
	\frametitle{Function Inverse Problem}
	
	\begin{equation*}
		\min_{\textcolor{red}{f}} L_h(u_h) \quad \mathrm{s.t.}\; F_h(\textcolor{red}{f}, u_h) = 0
	\end{equation*}
	
	What if the unknown is a \textcolor{red}{function} instead of a set of parameters?
\begin{itemize}
	\item Koopman operator in dynamical systems.
	\item Constitutive relations in solid mechanics. 
	\item Turbulent closure relations in fluid mechanics.
	\item ...
\end{itemize}

The candidate solution space is \textcolor{red}{infinite dimensional}.

\end{frame}


\begin{frame}
	\frametitle{Penalty Methods}
	
	\begin{itemize}
		\item Parametrize $f$ with $f_\theta$ and incorporate the physical constraint as a \textcolor{red}{penalty term} (regularization, prior, \ldots) in the loss function.
		\begin{equation*}
	\min_{\theta,\,u_h} L_h(u_h) + \lambda\|F_h(f_\theta, u_h)\|_2^2
		\end{equation*}
		\begin{itemize}
			\item May not satisfy physical constraint $F_h(f_\theta, u_h)=0$ accurately;
			\item Slow convergence for \textcolor{red}{stiff} problems;
			\begin{figure}[hbt]
				\includegraphics[width=0.6\textwidth]{figures/slowconvergence.png}
			\end{figure}
			
			
			\item High dimensional optimization problem; both $\theta$ and $u_h$ are variables.
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Machine Learning for Computational Engineering}
	\begin{enumerate}
		\item Approximate the unknown function with a \textcolor{red}{deep neural network}
		$$\min_{\theta} L_h(u_h) \quad \mathrm{s.t.}\;{F_h(\textcolor{red}{NN_\theta}, u_h) = 0}$$	
		\item Reduce the constrained optimization problem to an \textcolor{red}{unconstrained} optimization problem by solving the physical constraint numerically
		$$\min_{\theta} \tilde L_h(\theta) := L_h(u_h(\theta))$$
	\end{enumerate}

	
	
	\begin{center}
		 \textcolor{red}{Satisfy the physics to the largest extent}
	\end{center}
	\begin{figure}[hbt]
  \includegraphics[width=0.75\textwidth]{figures/physics_based_machine_learning.png}
\end{figure}
\end{frame}



\begin{frame}
	\frametitle{Gradient Based Optimization}
	\begin{equation}\label{equ:opt}
		\min_{\theta} \tilde L_h(\theta) := L_h(u_h(\theta))
		\end{equation}
	
	\begin{itemize}
		\item We can now apply a gradient-based optimization method to (\ref{equ:opt}).
		\item The key is to \textcolor{red}{calculate a descent direction} $g^k$
		$$\theta^{k+1} \gets \theta^k - \alpha g^k$$ 
	\end{itemize}
	
	\begin{figure}[hbt]
	\centering
  \includegraphics[width=0.6\textwidth]{figures/im.pdf}
\end{figure}

\end{frame}




\section{ADCME: Automatic Differentiation for Computational and Mathematical Engineering}


\begin{frame}
	\frametitle{Automatic Differentiation}
	The fact that bridges the \textcolor{red}{technical} gap between machine learning and inverse modeling:
	\begin{itemize}
		\item Deep learning (and many other machine learning techniques) and numerical schemes share the same computational model: composition of individual operators. 
	\end{itemize}
	
	
	\begin{minipage}[t]{0.4\textwidth}
		
		\
		
		
		
		\begin{center}
			\textcolor{red}{Mathematical Fact}
			
			\
			
			Back-propagation 
			
			$||$
			
			Reverse-mode
			
			Automatic Differentiation 
			
			$||$
			
			Discrete 
			
			Adjoint-State Method
		\end{center}
	\end{minipage}~
	\begin{minipage}[t]{0.6\textwidth}
		\begin{figure}[hbt]
			\includegraphics[width=0.8\textwidth]{figures/compare-NN-PDE.png}
		\end{figure}
	\end{minipage}
	
\end{frame}

\begin{frame}
	\frametitle{Computational Graph for Numerical Schemes}
	
	\begin{itemize}
		\item To leverage automatic differentiation for inverse modeling, we need to express the numerical schemes in the ``AD language'': computational graph. 
		\item No matter how complicated a numerical scheme is, it can be decomposed into a collection of operators that are interlinked via state variable dependencies. 
	\end{itemize}
	
	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/cgnum}
	\end{figure}
	
	
	
\end{frame}


\begin{frame}
	\frametitle{ADCME: Computational-Graph-based Numerical Simulation}
	
	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/custom}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{How ADCME works}
	\begin{itemize}
		\item ADCME translates your numerical simulation codes to computational graph and then the computations are delegated to a heterogeneous task-based parallel computing environment through TensorFlow runtime. 
	\end{itemize}
	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/routine2.png}
	\end{figure}
\end{frame}



\section{Distributed Computing via MPI}
\begin{frame}
	\frametitle{Parallel Computing}
	\begin{itemize}
		\item Parallel computing is essential for accelerating simulation and satisfying demanding memory requirements.
	\end{itemize}
	\begin{figure}[hbt]
		\includegraphics[width=0.8\textwidth]{figures/parallel}
	\end{figure}
\end{frame}




\begin{frame}
	\frametitle{Distributed Optimization}
	\begin{itemize}
		\item ADCME also supports MPI-based distributed computing. The parallel model is designed specially for scientific computing. 
		
		\begin{figure}[hbt]
			\centering
			\includegraphics[width=0.48\textwidth]{figures/distributed1}~
			\includegraphics[width=0.48\textwidth]{figures/distributed2}
		\end{figure}
		
		\item Key idea: \textcolor{red}{Everything is an operator}. Computation and communications are converters of data streams (tensors) through the computational graph. 
		
		\begin{center}
			\texttt{mpi\_bcast}, \texttt{mpi\_sum}, \texttt{mpi\_send}, \texttt{mpi\_recv}, \texttt{mpi\_halo\_exchange}, ...
		\end{center}
		
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Hybrid Parallel Computing}
	\begin{figure}
		\item We use \textbf{dependency injection} techniques to ensure consistency.
	\end{figure}
	\begin{figure}[hbt]
		\includegraphics[width=0.6\textwidth]{figures/adcme_parallel2.png}
	\end{figure}
\end{frame}






\begin{frame}
	\frametitle{Interoperability with Hypre}
	
	\begin{equation*}
		\begin{aligned}
			\nabla \cdot (\textcolor{red}{\textsf{NN}_\theta(\bx)} \nabla u(\bx)) & = f(\bx) & \bx \in \Omega \\ 
			u(\bx) &= 0 & \bx \in \partial \Omega
		\end{aligned}
	\end{equation*}
	The discretization leads to a linear system, which is solved using Hypre.
	
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/mpiscalability}
	\end{figure}
	
	
\end{frame}



\section{Physics Constrained Learning}

\begin{frame}
	
	
	\frametitle{Challenges in AD}
	
	
	\begin{minipage}{0.695\textwidth}
		\vspace{-6cm}
		\begin{itemize}
			\item Most AD frameworks only deal with \textcolor{red}{explicit operators}, i.e., the functions that has analytical derivatives, or composition of these functions. 
			\item Many scientific computing algorithms are \textcolor{red}{iterative} or \textcolor{red}{implicit} in nature.
			\vspace{1cm}
			{\small
			\begin{table}[]
				\begin{tabular}{@{}lll@{}}
					\toprule
					Linear/Nonlinear & Explicit/Implicit & Expression   \\ \midrule
					Linear           & Explicit          & $y=Ax$       \\
					Nonlinear        & Explicit          & $y = F(x)$   \\
					\textbf{Linear}           & \textbf{Implicit}          & $Ay = x$     \\
					\textbf{Nonlinear}        & \textbf{Implicit}          & $F(x,y) = 0$ \\ \bottomrule
				\end{tabular}
			\end{table}
		}
		\end{itemize}
	\end{minipage}~
	\begin{minipage}[t]{0.3\textwidth}
		\includegraphics[width=1.0\textwidth]{figures/implicitvsexplicit.png}
	\end{minipage}
	
	% Please add the following required packages to your document preamble:
	% \usepackage{booktabs}
	
\end{frame}


\begin{frame}
	\frametitle{Physics Constrained Learning (PCL)}
	$${\small    \min_{\theta}\; L_h(u_h) \quad \mathrm{s.t.}\;\; F_h(\theta, u_h) = 0}$$
	\begin{itemize}
		\item Assume that we solve for $u_h=G_h(\theta)$ with $F_h(\theta, u_h)=0$, and then
		$${\small\tilde L_h(\theta)  = L_h(G_h(\theta))}$$
		\item Applying the \textcolor{red}{implicit function theorem}
		{  \scriptsize
			\begin{equation*}
				\frac{{\partial {F_h(\theta, u_h)}}}{{\partial \theta }} + {\frac{{\partial {F_h(\theta, u_h)}}}{{\partial {u_h}}}}
				\textcolor{red}{\frac{\partial G_h(\theta)}{\partial \theta}}
				= 0 \Rightarrow
				\textcolor{red}{\frac{\partial G_h(\theta)}{\partial \theta}} =  -\Big( \frac{{\partial {F_h(\theta, u_h)}}}{{\partial {u_h}}} \Big)^{ - 1} \frac{{\partial {F_h(\theta, u_h)}}}{{\partial \theta }}
			\end{equation*}
		}
		\item Finally we have
		{\scriptsize
			\begin{equation*}
				\boxed{\frac{{\partial {{\tilde L}_h}(\theta )}}{{\partial \theta }}
					= \frac{\partial {{ L}_h}(u_h )}{\partial u_h}\frac{\partial G_h(\theta)}{\partial \theta}=
					- \textcolor{red}{ \frac{{\partial {L_h}({u_h})}}{{\partial {u_h}}} } \;
					\textcolor{blue}{ \Big( {\frac{{\partial {F_h(\theta, u_h)}}}{{\partial {u_h}}}\Big|_{u_h = {G_h}(\theta )}} \Big)^{ - 1} } \;
					\textcolor{ForestGreen}{ \frac{{\partial {F_h(\theta, u_h)}}}{{\partial \theta }}\Big|_{u_h = {G_h}(\theta )} }
				}
			\end{equation*}
		}
		
	\end{itemize}
	
\end{frame}





\begin{frame}
	\frametitle{Theoretical Analysis}
	
	\begin{itemize}
		\item For stiff problems, better to resolve physics using PCL.
		\item Consider a model problem 
		\begin{gather*}
			\min_{\theta} \|u-u_0\|^2_2 \qquad \text{s.t.} \;\; Au = \theta y
		\end{gather*}
	\vspace{-0.5cm}
	\begin{align*}
		\text{PCL}: &&\ \min_\theta \tilde L_h(\theta) &= \|\theta A^{-1} y - u_0\|^2_2 = (\theta-1)^2\|u_0\|_2^2\\
				\text{Penalty Method}: &&\ 
			\min_{\theta, u_h}\tilde L_h(\theta, u_h) &= \|u_h-u_0\|^2_2 + \lambda \|Au_h -\theta y\|_2^2
	\end{align*}
\begin{theorem}
	The condition number of $\mathbf{A}_\lambda$ is 
	\begin{equation*}
		\liminf_{\lambda\rightarrow \infty}\kappa(\mathbf{A}_\lambda)  \geq  \kappa(A)^2,\qquad \mathbf{A}_\lambda = \begin{bmatrix}
			I & 0\\
			\sqrt{\lambda}A & -\sqrt{\lambda}y
		\end{bmatrix}, \qquad 
		\mathbf{y} = \begin{bmatrix}
			u_0\\ 0
		\end{bmatrix}
	\end{equation*}
	and therefore, the condition number of the unconstrained optimization problem from the penalty method is equal to the the square of the condition number of the PCL asymptotically. 
\end{theorem}
 	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Physics Constrained Learning for Stiff Problems}
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=1.0\textwidth]{figures/pcl.png}
	\end{figure}
\end{frame}



\begin{frame}
	\frametitle{PCL: Backbone of the ADCME Infrastructure}
	
	
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.9\textwidth]{figures/infrastructure.png}
	\end{figure}
\end{frame}


\section{Applications: Constitutive Modeling}




\begin{frame}
	\frametitle{Governing Equations}
	
	\begin{equation}\label{equ:momentum}
		\begin{aligned}
			\underbrace{\sigma_{ij,j}}_{\mbox{stress}} + \rho \underbrace{b_i}_{\mbox{external force}} &= \rho \underbrace{\ddot u_i}_{\mbox{velocity}}\\
			\underbrace{\varepsilon_{ij}}_{\mbox{strain}} &= \frac{1}{2}(u_{j,i}+u_{i,j})
		\end{aligned}
	\end{equation}
	
	
	\begin{itemize}
		\item \textbf{Observable}: external/body force $b_i$, displacements $u_i$ (strains $\varepsilon_{ij}$ can be computed from $u_i$); density $\rho$ is known.  
		\item \textbf{Unobservable}: stress $\sigma_{ij}$. 
		\item Data-driven Constitutive Relations: modeling the strain-stress relation using a neural network
		\begin{equation}\label{equ:nn}
			\boxed{\mbox{stress} =\mathcal{M}_{\theta}(\mbox{strain},\ldots)}
		\end{equation}
		and the neural network is trained by coupling Eq.~\ref{equ:momentum} and Eq.~\ref{equ:nn}.
	\end{itemize}
	
	
\end{frame}

\begin{frame}
	\frametitle{Representation of Constitutive Relations}
	\begin{itemize}
		\item Proper form of constitutive relation is crucial for numerical stability
		{\footnotesize\begin{align*}
				\mbox{Elasticity} &\Rightarrow \bm\sigma = \mathsf{C}_{\theta}\bm\epsilon \\
				\mbox{Hyperelasticity } &\Rightarrow \begin{cases}\bm\sigma =\mathcal{M}_{\theta}(\bm\epsilon) & \mbox{(Static)} \\
					\bm{\sigma}^{n+1}  =  \ChoL_{\bt}(\bm\epsilon^{n+1}) \ChoL_{\bt}(\bm\epsilon^{n+1})^T (\bm{\epsilon}^{n+1} - \bm{\epsilon}^{n})  + \bm{\sigma}^{n}  & \mbox{(Dynamic)} \end{cases} \\
				\mbox{Elaso-Plasticity} &\Rightarrow \bm\sigma^{n+1} = \ChoL_{\bt}(\bm\epsilon^{n+1},\bm{\epsilon}^{n},\bm{\sigma}^{n}) \ChoL_{\bt}(\bm\epsilon^{n+1},\bm{\epsilon}^{n},\bm{\sigma}^{n})^T (\bm{\epsilon}^{n+1} - \bm{\epsilon}^{n})  + \bm{\sigma}^{n} 
			\end{align*}
		}{\footnotesize$$\ChoL_{\bt} = \begin{bmatrix}
				L_{1111}  &  & &  &       &\\
				L_{2211}  & L_{2222} & &   & &\\
				L_{3311}  &  L_{3322}               & L_{3333} &  & &\\
				&                 &                 & L_{2323}&  &\\
				&               &                  &                & L_{1313} &\\
				&                 &                  &                &                 &L_{1212}\\
			\end{bmatrix}$$}
		\item \textcolor{red}{Weak convexity}: $\ChoL_{\bt}\ChoL_{\bt}^T \succ 0$
		\item \textcolor{red}{Time consistency}:  $\bm\sigma^{n+1} \rightarrow \bm \sigma^n$ when $\bm\epsilon^{n+1} \rightarrow \bm \epsilon^n$
	\end{itemize}
	
\end{frame}


\begin{frame}
	\frametitle{Modeling Elasto-plasticity}
	\begin{itemize}
		\item Comparison of different neural network architectures 
		\begin{align*}
			\bm\sigma^{n+1} &= \ChoL_{\bt}(\bm\epsilon^{n+1},\bm{\epsilon}^{n},\bm{\sigma}^{n}) \ChoL_{\bt}(\bm\epsilon^{n+1},\bm{\epsilon}^{n},\bm{\sigma}^{n})^T (\bm{\epsilon}^{n+1} - \bm{\epsilon}^{n})  + \bm{\sigma}^{n} \\
			\bm{\sigma}^{n+1} &=  \mathsf{NN}_{\bt}(\bm\epsilon^{n+1},\bm{\epsilon}^{n},\bm{\sigma}^{n})\\
			\bm{\sigma}^{n+1} &=  \mathsf{NN}_{\bt}(\bm\epsilon^{n+1},\bm{\epsilon}^{n},\bm{\sigma}^{n}) + \bm{\sigma}^{n}
		\end{align*}
	\end{itemize}
	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/nncons}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Modeling Elasto-plasticity: Multi-scale}
	\begin{figure}[hbt]
		\includegraphics[width=0.8\textwidth]{figures/plasticity}
	\end{figure}
\end{frame}


\begin{frame}
	\frametitle{Other Applications}
	
	\begin{itemize}
		\item Time‐Lapse Full‐Waveform Inversion for Subsurface Flow;
		\item Seismic Inversion;
		\item Viscoelasticity Modeling;
		\item Seismic Inversion;
		\item Stochastic Differential Equations;
		\item Navier Stokes Equations;
		\item $\ldots$
	\end{itemize}

See the following slide for more details:

\begin{center}
	\url{https://kailaix.github.io/ADCMESlides/2020_11_17.pdf}
\end{center}
\end{frame}




\begin{frame}
	\frametitle{A General Approach to Inverse Modeling}
	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/summary.png}
	\end{figure}
\end{frame}


%}
%\usebackgroundtemplate{}
%----------------------------------------------------------------------------------------
%    PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

%------------------------------------------------



\end{document} 