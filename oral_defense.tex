%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%    PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass[usenames,dvipsnames]{beamer}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{algorithm}

\usepackage{animate}
\usepackage{float}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{extarrows}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{minted}

\newcommand{\uc}{\underline{c}}
\newcommand{\ChoL}{\mathsf{L}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\ii}{\mathrm{i}}
\newcommand{\bxi}{\bm{\xi}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bM}{\mathbf{M}}

\newcommand{\by}{\mathbf{y}}
\newcommand{\bw}{\mathbf{w}}

\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\bt}[0]{\bm{\theta}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bzero}{\mathbf{0}}
\renewcommand{\bf}{\mathbf{f}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}[0]{\mathbf{v}}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}


% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{soul}
\newcommand{\red}[1]{\textcolor{red}{#1}}
%
%\usepackage{graphicx} % Allows including images
%\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
%
%
%\usepackage{amsthm}
%
%\usepackage{todonotes}
%\usepackage{floatrow}
%
%\usepackage{pgfplots,algorithmic,algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

%\usepackage[toc,page]{appendix}
%\usepackage{float}
%\usepackage{booktabs}
%\usepackage{bm}
%
%\theoremstyle{definition}
%
\newcommand{\RR}[0]{\mathbb{R}}
%
%\newcommand{\bx}{\mathbf{x}}
%\newcommand{\ii}{\mathrm{i}}
%\newcommand{\bxi}{\bm{\xi}}
%\newcommand{\bmu}{\bm{\mu}}
%\newcommand{\bb}{\mathbf{b}}
%\newcommand{\bA}{\mathbf{A}}
%\newcommand{\bJ}{\mathbf{J}}
%\newcommand{\bB}{\mathbf{B}}
%\newcommand{\bM}{\mathbf{M}}
%\newcommand{\bF}{\mathbf{F}}
%
%\newcommand{\by}{\mathbf{y}}
%\newcommand{\bw}{\mathbf{w}}
%\newcommand{\bn}{\mathbf{n}}
%
%\newcommand{\bX}{\mathbf{X}}
%\newcommand{\bY}{\mathbf{Y}}
%\newcommand{\bs}{\mathbf{s}}
%\newcommand{\sign}{\mathrm{sign}}
%\newcommand{\bt}[0]{\bm{\theta}}
%\newcommand{\bc}{\mathbf{c}}
%\newcommand{\bzero}{\mathbf{0}}
%\renewcommand{\bf}{\mathbf{f}}
%\newcommand{\bu}{\mathbf{u}}
%\newcommand{\bv}[0]{\mathbf{v}}
\usepackage{etoolbox}
\AtBeginSection[]
{
	\ifnumcomp{\value{section}}{=}{1}{
	
	\begin{frame}
		\frametitle{Outline}
		\tableofcontents[sectionstyle=show,hideothersubsections]
	\end{frame}

	}{
		
	}


}


%----------------------------------------------------------------------------------------
%    TITLE PAGE
%----------------------------------------------------------------------------------------
\usepackage{bm}
\newcommand*{\TakeFourierOrnament}[1]{{%
\fontencoding{U}\fontfamily{futs}\selectfont\char#1}}
\newcommand*{\danger}{\TakeFourierOrnament{66}}

\title[Kailai Xu]{Machine Learning for Computational Engineering} % The short title appears at the bottom of every slide, the full title is only on the title page

\author[Kailai Xu]{Kailai Xu \\ Stanford University} % Your name


\setbeamertemplate{footline}{
	\hbox{%
		\begin{beamercolorbox}[wd=\paperwidth,ht=3ex,dp=1.5ex,leftskip=2ex,rightskip=2ex]{page footer}%
			\usebeamerfont{title in head/foot}%
			\insertshorttitle \hfill
			\insertsection \hfill
			\insertframenumber{} / \inserttotalframenumber
	\end{beamercolorbox}}%
}
%\institute[] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
%{
%%ICME, Stanford University \\ % Your institution for the title page
%%\medskip
%%\textit{kailaix@stanford.edu}\quad \textit{darve@stanford.edu} % Your email address
%}
\date{}% Date, can be changed to a custom date
% Mathematics of PDEs

	\usepackage{cleveref}
\begin{document}


\usebackgroundtemplate{%
\begin{picture}(0,250)
\centering
	{{\includegraphics[width=1.0\paperwidth]{figures/background}}}
\end{picture}
  } 
%\usebackgroundtemplate{%
%  \includegraphics[width=\paperwidth,height=\paperheight]{figures/back}} 
\begin{frame}

\titlepage % Print the title page as the first slide

%dfa
\end{frame}
\usebackgroundtemplate{}

\section{Inverse Modeling}



\begin{frame}
	\frametitle{Inverse Modeling}
	\begin{figure}
		\centering
		\includegraphics[width=1.0\textwidth]{figures/inverse3}
	\end{figure}
\end{frame}



\begin{frame}
	\frametitle{Inverse Modeling}
	We can formulate inverse modeling as a PDE-constrained optimization problem 
	\begin{equation*}
		\min_{\theta} L_h(u_h) \quad \mathrm{s.t.}\; F_h(\theta, u_h) = 0
	\end{equation*}
	\begin{itemize}
		\item The \textcolor{red}{loss function} $L_h$ measures the discrepancy between the prediction $u_h$ and the observation $u_{\mathrm{obs}}$, e.g., $L_h(u_h) = \|u_h - u_{\mathrm{obs}}\|_2^2$. 
		\item $\theta$ is the \textcolor{red}{model parameter} to be calibrated. 
		\item The \textcolor{red}{physics constraints} $F_h(\theta, u_h)=0$ are described by a system of partial differential equations or differential algebraic equations (DAEs); e.g., 
		$$F_h(\theta, u_h) = \mathbf{A}(\theta) u_h - f_h = 0$$
	\end{itemize}
\end{frame}




\begin{frame}
	\frametitle{Function Inverse Problem}
	
	\begin{equation*}
		\min_{\textcolor{red}{f}} L_h(u_h) \quad \mathrm{s.t.}\; F_h(\textcolor{red}{f}, u_h) = 0
	\end{equation*}
	
	What if the unknown is a \textcolor{red}{function} instead of a set of parameters?
\begin{itemize}
	\item Koopman operator in dynamical systems.
	\item Constitutive relations in solid mechanics. 
	\item Turbulent closure relations in fluid mechanics.
	\item ...
\end{itemize}

The candidate solution space is \textcolor{red}{infinite dimensional}.

\end{frame}





\begin{frame}
	\frametitle{Machine Learning for Computational Engineering}
	$$\min_{\theta} L_h(u_h) \quad \mathrm{s.t.}\;\boxed{F_h(\textcolor{red}{\text{N}_\theta}, u_h) = 0} \leftarrow \mbox{Solved numerically}$$
	\vspace{-0.5cm}
	\begin{enumerate}
		\item Use a deep neural network to approximate the (high dimensional) unknown function;
		\item Solve $u_h$ from the physical constraint using a \textcolor{red}{numerical PDE solver};
		\item Apply an unconstrained optimizer to the reduced problem
		$$\min_{\theta} L_h(\textcolor{red}{u_h(\theta)})$$
	\end{enumerate}
	\vspace{-0.3cm}
	\begin{figure}[hbt]
		\includegraphics[width=0.75\textwidth]{figures/physics_based_machine_learning.png}
	\end{figure}
\end{frame}



\begin{frame}
	\frametitle{Gradient Based Optimization}
		$$\min_{\theta} L_h({u_h(\theta)})$$
	
	\begin{itemize}
		\item Steepest descent method:
		$$\theta_{k+1} \gets \theta_k - \alpha_k \nabla_\theta L_h(u_h(\theta_k))$$ 
	\end{itemize}
	
	\begin{figure}[hbt]
	\centering
  \includegraphics[width=0.6\textwidth]{figures/thesis/workflow.pdf}
\end{figure}

\end{frame}


\begin{frame}{Contributions}
	
	\textbf{Goal}
	
	\vspace{0.2cm}
	
	\begin{quote}
	Develop algorithms and tools for solving inverse problems by \\ combining DNNs and numerical PDE solvers. 
	\end{quote}

	
	\begin{figure}[hbt]
	\centering
	\includegraphics[width=1.0\textwidth]{figures/thesis/outline.pdf}
\end{figure}
\end{frame}



\section{Software Implementation}

\begin{frame}	
	
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=1.0\textwidth]{figures/thesis/outline1}
	\end{figure}
\end{frame}


\begin{frame}
	\frametitle{Ecosystem for Inverse Modeling}
	
	
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.9\textwidth]{figures/thesis/eco}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Applications}
	
	
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.9\textwidth]{figures/thesis/pub}
	\end{figure}
\end{frame}


\begin{frame}
	\frametitle{Applications: Solid Mechanics}
	\begin{itemize}
\item Modeling constitutive relations with deep neural networks

	\end{itemize}
		\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/thesis/rnn.pdf}
	\includegraphics[width=0.33\textwidth]{figures/thesis/plate_multiscale_stress_reference300.png}~
	\includegraphics[width=0.33\textwidth]{figures/thesis/plate_multiscale_stress_test_nntrain2_piecewise_from3_test300.pdf}
		\end{figure}
			
			\vspace{-0.2cm}
		{
			\tiny
			
			\begin{itemize}
				\item[] Kailai Xu*, Daniel Z. Huang*, and Eric Darve. \textit{Learning constitutive relations using symmetric positive definite neural networks}. Journal of Computational Physics 428 (2021): 110072.
				\item[] Daniel Z. Huang*, Kailai Xu*, Charbel Farhat, and Eric Darve. \textit{Learning constitutive relations from indirect observations using deep neural networks}. Journal of Computational Physics 416 (2020): 109491.
			\end{itemize}
			
			
			
		}
\end{frame}

\begin{frame}
	\frametitle{Applications: Seismic Inversion}
	\begin{itemize}
		\item \textbf{ADSeismic}: AD $+$ Seismic Inversion
		\item \textbf{NNFWI}: DNN $+$ FWI
	\end{itemize}

\begin{minipage}[t]{0.7\textwidth}
		\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/adseimic.jpeg}
	\end{figure}
\end{minipage}~
\begin{minipage}[t]{0.3\textwidth}
	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/thesis/marmousi_large_true}
				\includegraphics[width=1.0\textwidth]{figures/thesis/FWI_marmousi_large}
						\includegraphics[width=1.0\textwidth]{figures/thesis/NNFWI_marmousi_large}
	\end{figure}
\end{minipage}

		
	\vspace{-0.2cm}
	{
		\tiny
		
		\begin{itemize}
			\item[] Weiqiang Zhu*, Kailai Xu*, Eric Darve, and Gregory C. Beroza. \textit{A general approach to seismic inversion with automatic differentiation}. Computers \& Geosciences (2021): 104751.
			\item[] Weiqiang Zhu*, Kailai Xu*, Eric Darve, Biondo Biondi, and Gregory C. Beroza. \textit{Integrating Deep Neural Networks with Full-waveform Inversion: Reparametrization, Regularization, and Uncertainty Quantification}. Submitted.
		\end{itemize}
		
		
		
	}
\end{frame}

\begin{frame}
	\frametitle{Applications: Fluid Dynamics}
	
	\begin{figure}[hbt]
		\centering 
		\includegraphics[width=0.7\textwidth]{figures/advertisement.png}
	\end{figure}

	\vspace{-0.2cm}
{
	\tiny
	
	\begin{itemize}
		\item[] Tiffany Fan, Kailai Xu, Jay Pathak, and Eric Darve. \textit{Solving Inverse Problems in Steady State Navier-Stokes Equations using Deep Neural Networks}. PGAI-AAAI (2020)
	\end{itemize}
}

\end{frame}

\begin{frame}
	\frametitle{Applications: Geo-mechanics}
	
	\begin{itemize}
	\item Learning intrinsic fluid properties from indirect seismic data using automatic differentiation
	\item Modeling viscoelasticity using deep neural networks 
	\end{itemize}

	\begin{figure}[hbt]
	\centering 
	\includegraphics[width=0.42\textwidth]{figures/thesis/diagram.png}~
	\includegraphics[width=0.42\textwidth]{figures/visco2.png}
\end{figure}
	\vspace{-0.2cm}
{
	\tiny
	
	\begin{itemize}
		\item[] Dongzhuo Li*, Kailai Xu*, Jerry M. Harris, and Eric Darve. \textit{Coupled Time‐Lapse Full‐Waveform Inversion for Subsurface Flow Problems Using Intrusive Automatic Differentiation}. Water Resources Research 56, no. 8 (2020): e2019WR027032.
		
		\item[] Kailai Xu, Alexandre M. Tartakovsky, Jeff Burghardt, and Eric Darve. \textit{Learning Viscoelasticity Models from Indirect Data using Deep Neural Networks}. Submitted.
	\end{itemize}
}

\end{frame}



\begin{frame}
	\frametitle{Applications: Stochastic Processes}
	\begin{itemize}
		\item Approximating unknown distributions with deep neural networks in a stochastic process/differential equation. 
		\begin{itemize}
			\item \textbf{Adversarial Inverse Modeling (AIM)}: adversarial training
			\item \textbf{Physics Generative Neural Networks (PhysGNN)}: optimal transport
		\end{itemize}
	\end{itemize}
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.59\textwidth]{figures/thesis/ill1}~				\includegraphics[width=0.39\textwidth]{figures/thesis/ana}
	\end{figure}
	
		\vspace{-0.2cm}
	{
		\tiny
		
		\begin{itemize}
			\item[] 	Kailai Xu and Eric Darve. \textit{Solving Inverse Problems in Stochastic Models using Deep NeuralNetworks and Adversarial Training}. Submitted.
			
			\item[] 	Kailai Xu, Weiqiang Zhu, and Eric Darve. \textit{Learning Generative Neural Networks with Physics Knowledge}. Submitted.
		\end{itemize}
	}
	
\end{frame}

\begin{frame}
	\frametitle{Automatic Differentiation}
	Bridging the technical gap between deep learning and inverse modeling: 
	
	
	\begin{minipage}[t]{0.4\textwidth}
		
		\
		
		
		
		\begin{center}
			\textcolor{red}{Mathematical Fact}
			
			\
			
			Back-propagation 
			
			$||$
			
			Reverse-mode
			
			Automatic Differentiation 
			
			$||$
			
			Discrete 
			
			Adjoint-State Method
		\end{center}
	\end{minipage}~
	\begin{minipage}[t]{0.6\textwidth}
		\begin{figure}[hbt]
			\includegraphics[width=0.8\textwidth]{figures/compare-NN-PDE.png}
		\end{figure}
	\end{minipage}
	
\end{frame}

\begin{frame}
	\frametitle{Computational Graph for Numerical Schemes}
	
	\begin{itemize}
		\item To leverage automatic differentiation for inverse modeling, we need to express the numerical schemes in the ``AD language'': computational graph. 
		\item No matter how complicated a numerical scheme is, it can be decomposed into a collection of operators that are interlinked via state variable dependencies. 
	\end{itemize}
	
	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/cgnum}
	\end{figure}
	
	
	
\end{frame}


\begin{frame}
	\frametitle{ADCME: Computational-Graph-based Numerical Simulation}
	
	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/custom}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{How ADCME works}
	\begin{itemize}
		\item ADCME translates your numerical simulation codes to computational graph and then the computations are delegated to a heterogeneous task-based parallel computing environment through TensorFlow runtime. 
	\end{itemize}
	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/routine2.png}
	\end{figure}
\end{frame}

\begin{frame}{Summary}

\begin{itemize}
\item Mathematically equivalent techniques for calculating gradients:

\begin{itemize}
\item gradient back-propagation (DNN)
\item discrete adjoint-state methods (PDE)
\item reverse-mode automatic differentiation
\end{itemize}

\item Computational graphs bridge the gap between gradient calculations in numerical PDE solvers and DNNs. 

\item ADCME extends the capability of TensorFlow to PDE solvers, providing users a single piece of software for numerical simulations, deep learning, and optimization.


\end{itemize}

\end{frame}

\section{First Order Physics Constrained Learning}

\begin{frame}	
	
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=1.0\textwidth]{figures/thesis/outline2}
	\end{figure}
\end{frame}

\begin{frame}
	
	
	\frametitle{Motivation}
	
	
	\begin{minipage}{0.695\textwidth}
		\vspace{-6cm}
		\begin{itemize}
			\item Most AD frameworks only deal with \textcolor{red}{explicit operators}, i.e., the functions that has analytical derivatives, or composition of these functions. 
			\item Many scientific computing algorithms are \textcolor{red}{iterative} or \textcolor{red}{implicit} in nature.
			\vspace{1cm}
			{\small
			\begin{table}[]
				\begin{tabular}{@{}lll@{}}
					\toprule
					Linear/Nonlinear & Explicit/Implicit & Expression   \\ \midrule
					Linear           & Explicit          & $y=Ax$       \\
					Nonlinear        & Explicit          & $y = F(x)$   \\
					\textbf{Linear}           & \textbf{Implicit}          & $Ay = x$     \\
					\textbf{Nonlinear}        & \textbf{Implicit}          & $F(x,y) = 0$ \\ \bottomrule
				\end{tabular}
			\end{table}
		}
		\end{itemize}
	\end{minipage}~
	\begin{minipage}[t]{0.3\textwidth}
		\includegraphics[width=1.0\textwidth]{figures/implicitvsexplicit.png}
	\end{minipage}
	
	% Please add the following required packages to your document preamble:
	% \usepackage{booktabs}
	
\end{frame}

\begin{frame}
	\frametitle{Example}
	
	\begin{itemize}
		\item Consider a function $f:x\rightarrow y$, which is implicitly defined by 
		$$F(x,y) = x^3 - (y^3+y) = 0$$
		If not using the cubic formula for finding the roots, the forward computation consists of iterative algorithms, such as the Newton's method and bisection method
	\end{itemize}
	
	
	
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\begin{algorithmic}
			\State $y^0 \gets 0$
			\State $k \gets 0$
			\While {$|F(x, y^k)|>\epsilon$}
			\State $\delta^k \gets F(x, y^k)/F'_y(x,y^k)$
			\State $y^{k+1}\gets y^k - \delta^k$
			\State $k \gets k+1$
			\EndWhile
			\State \textbf{Return} $y^k$
		\end{algorithmic}
	\end{minipage}~
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\begin{algorithmic}
			\State $l \gets -M$, $r\gets M$, $m\gets 0$
			\While {$|F(x, m)|>\epsilon$}
			\State $c \gets \frac{a+b}{2}$
			\If{$F(x, m)>0$}
			\State $a\gets m$
			\Else
			\State $b\gets m$
			\EndIf
			\EndWhile
			\State \textbf{Return} $c$
		\end{algorithmic}
		
	\end{minipage}	
	
\end{frame}

\begin{frame}
	\frametitle{Example}
	
	\begin{itemize}
		%		\item A simple approach is to save part or all intermediate steps, and ``back-propagate''. This approach is expensive in both computation and memory\footnote{Ablin, Pierre, Gabriel Peyr�, and Thomas Moreau. ``Super-efficiency of automatic differentiation for functions defined as a minimum.''}.
		%		\item Nevertheless, the simple approach works in some scenarios where accuracy or cost is not an issue, e.g., automatic differetiation of soft-DTW and Sinkhorn distance. 
		\item An efficient way to do automatic differentiation is to apply the \textcolor{red}{implicit function theorem}. For our example, $F(x,y)=x^3-(y^3+y)=0$; treat $y$ as a function of $x$ and take the derivative on both sides
		$$3x^2 - 3y(x)^2y'(x)-y'(x)=0\Rightarrow y'(x) = \frac{3x^2}{3y^2+1}$$
		The above gradient is \textcolor{red}{exact}.
	\end{itemize}
	\begin{center}
		\textbf{Can we apply the same idea to inverse modeling?}
	\end{center}
	
\end{frame}



\begin{frame}
	\frametitle{Physics Constrained Learning (PCL)}
	$${\small    \min_{\theta}\; L_h(u_h) \quad \mathrm{s.t.}\;\; F_h(\theta, u_h) = 0}$$
	\begin{itemize}
		\item Assume that we solve for $u_h=G_h(\theta)$ with $F_h(\theta, u_h)=0$, and then
		$${\small\tilde L_h(\theta)  = L_h(G_h(\theta))}$$
		\item Applying the \textcolor{red}{implicit function theorem}
		{  \scriptsize
			\begin{equation*}
				\frac{{\partial {F_h(\theta, u_h)}}}{{\partial \theta }} + {\frac{{\partial {F_h(\theta, u_h)}}}{{\partial {u_h}}}}
				\textcolor{red}{\frac{\partial G_h(\theta)}{\partial \theta}}
				= 0 \Rightarrow
				\textcolor{red}{\frac{\partial G_h(\theta)}{\partial \theta}} =  -\Big( \frac{{\partial {F_h(\theta, u_h)}}}{{\partial {u_h}}} \Big)^{ - 1} \frac{{\partial {F_h(\theta, u_h)}}}{{\partial \theta }}
			\end{equation*}
		}
		\item Finally we have
		{\scriptsize
			\begin{equation*}
				\boxed{\frac{{\partial {{\tilde L}_h}(\theta )}}{{\partial \theta }}
					= \frac{\partial {{ L}_h}(u_h )}{\partial u_h}\frac{\partial G_h(\theta)}{\partial \theta}=
					- \textcolor{red}{ \frac{{\partial {L_h}({u_h})}}{{\partial {u_h}}} } \;
					\textcolor{blue}{ \Big( {\frac{{\partial {F_h(\theta, u_h)}}}{{\partial {u_h}}}\Big|_{u_h = {G_h}(\theta )}} \Big)^{ - 1} } \;
					\textcolor{ForestGreen}{ \frac{{\partial {F_h(\theta, u_h)}}}{{\partial \theta }}\Big|_{u_h = {G_h}(\theta )} }
				}
			\end{equation*}
		}
		
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Penalty Methods}
	\begin{equation*}
		\min_{\textcolor{red}{f}} L_h(u_h) \quad \mathrm{s.t.}\; F_h(\textcolor{red}{f}, u_h) = 0
	\end{equation*}
	\begin{itemize}
		\item \textcolor{red}{Penalty Method}: parametrize $f$ with $f_\theta$ (DNNs, linear finite element basis, radial basis functions, etc.) and incorporate the physical constraint as a \textcolor{red}{penalty term} (regularization, prior, \ldots) in the loss function.
		\begin{equation*}
			\min_{\theta,\,u_h} L_h(u_h) + \lambda\|F_h(f_\theta, u_h)\|_2^2
		\end{equation*}
		\begin{itemize}
			\item[+] Easy to implement (no need for differentiating numerical solvers) 
			\item[$-$] May not satisfy physical constraint $F_h(f_\theta, u_h)=0$ accurately;
			
			\item[$-$] High dimensional optimization problem; both $\theta$ and $u_h$ are variables.
		\end{itemize}
	\end{itemize}
\end{frame}




\begin{frame}
	\frametitle{Physics Constrained Learning for Stiff Problems}
	
	\begin{itemize}
		\item \textcolor{red}{PCL is superior for stiff problems.}
		\item Consider a model problem 
		\begin{gather*}
			\min_{\theta} \|u-u_0\|^2_2 \qquad \text{s.t.} \;\; Au = \theta y
		\end{gather*}
	\vspace{-0.5cm}
	\begin{align*}
		\text{PCL}: &&\ \min_\theta \tilde L_h(\theta) &= \|\theta A^{-1} y - u_0\|^2_2 = (\theta-1)^2\|u_0\|_2^2\\
				\text{Penalty Method}: &&\ 
			\min_{\theta, u_h}\tilde L_h(\theta, u_h) &= \|u_h-u_0\|^2_2 + \lambda \|Au_h -\theta y\|_2^2
	\end{align*}
\vspace{-0.5cm}
\begin{theorem}
	The condition number of $\mathbf{A}_\lambda$ is 
	\begin{equation*}
		\liminf_{\lambda\rightarrow \infty}\kappa(\mathbf{A}_\lambda)  =  \kappa(A)^2,\qquad \mathbf{A}_\lambda = \begin{bmatrix}
			I & 0\\
			\sqrt{\lambda}A & -\sqrt{\lambda}y
		\end{bmatrix}, \qquad 
		\mathbf{y} = \begin{bmatrix}
			u_0\\ 0
		\end{bmatrix}
	\end{equation*}
	and therefore, the condition number of the unconstrained optimization problem from the penalty method is equal to the square of the condition number of the PCL asymptotically. 
\end{theorem}
 	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Physics Constrained Learning for Stiff Problems}
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=1.0\textwidth]{figures/pcl.png}
	\end{figure}
\end{frame}


\begin{frame}{Summary}
\begin{itemize}
\item Implicit and iterative operators are ubiquitous in numerical PDE solvers. These operators are insufficiently treated in deep learning software and frameworks.

\item PCL helps you calculate gradients of implicit/iterative operators efficiently. 

\item PCL leads to faster convergence and better accuracy compared to penalty methods for stiff problems.
\end{itemize}
\end{frame}

\section{Second Order Physics Constrained Learning}



\begin{frame}	
	
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=1.0\textwidth]{figures/thesis/outline3}
	\end{figure}
\end{frame}


\begin{frame}{Motivation}


	
\begin{figure}
	\centering
	\includegraphics[width=1.0\textwidth]{figures/thesis/optimization.png}
\end{figure}




\end{frame}

\begin{frame}{Overview}
		\begin{figure}[htbp]
		\centering
		\includegraphics[width=0.5\textwidth]{figures/thesis/losses2_dynamic.pdf}
	\end{figure}
	
	\vspace{-0.4cm} 
	
	\textbf{Goal}
	
	\vspace{0.2cm} 
	
\begin{quote}
	Accelerate convergence and improve accuracy with Hessian information
\end{quote}

\textbf{Challenge}

\vspace{0.2cm} 

\begin{quote}
Calculate Hessians for coupled systems of PDEs and DNNs
\end{quote}

\end{frame}

\begin{frame}{Trust Region vs. Line Search}
	
	
	\begin{minipage}{0.6\textwidth}
		\textbf{Trust Region}
		
		\small
		
		\begin{itemize}
			\item Approximate $f(x_k + p)$ by a model quadratic function 
			$$m_k(p) = f_k + g_k^T p + \frac{1}{2}p^T B_k p$$
			$${\small f_k = f(x_k), g_k = \nabla f(x_k), B_k = \nabla^2 f(x_k)}$$
			\item Solve  the optimization problem within a trust region  $\|p\|\leq \Delta_k$
			\begin{equation*}
				p_k = \arg\min_{p} \; m_k(p) \quad	\text{s.t.} \; \|p\| \leq \Delta_k 
			\end{equation*}
			\vspace{-0.5cm}
			\item If decrease in $f(x_k + p_k)$ is sufficient, then update the state $x_{k+1} = x_k +  p_k$; otherwise, $x_{k+1} = x_k$ and improve $\Delta_k$.
		\end{itemize}
		
	\end{minipage}~
	\begin{minipage}{0.4\textwidth}
		\begin{figure}
			\centering
			\includegraphics[width=1.0\textwidth]{figures/thesis/tr_ls.PNG}
		\end{figure}
		
		\textbf{Line Search}
		
		\small
		\begin{itemize}
			\item Determine a descent direction $p_k$
			\item Determine a step size $\alpha_k$ that sufficiently reduces $f(x_k + \alpha_k p_k)$
			\item Update the state $x_{k+1} = x_k + \alpha_k p_k$
		\end{itemize}
	\end{minipage}
	
	
	
	
\end{frame}


\begin{frame}{Second Order Physics Constrained Learning}
	\begin{itemize}
\item Consider a composite function with a vector input $x$ and scalar output 
\begin{equation}\label{equ:t1}
	v = f(G(x))
\end{equation}
\item Define 
{\small
\begin{align*}
	f_{,k}(y) &= \frac{\partial f(y)}{\partial y_k}, \quad f_{,kl}(y) = \frac{\partial^2 f(y)}{\partial y_k \partial y_l} \\ 
	G_{k,l}(x) &=\frac{\partial G_k(x)}{\partial x_l},\quad G_{k, lr}(x) = \frac{\partial^2 G_k(x)}{\partial x_l\partial x_r}
\end{align*}
}
\item Differentiate \Cref{equ:t1} with respect to $x_i$
\begin{equation}\label{equ:t2}
\frac{\partial v}{\partial x_i} = f_{,k}G_{k,i}
\end{equation}
\item Differentiate \Cref{equ:t2} with respect to $x_j$
$$\boxed{\frac{\partial^2 v}{\partial x_i \partial x_j} = f_{,kr} G_{k,i} G_{r,j} + f_{,k} G_{k,ij}}$$
	\end{itemize}

\end{frame}


\begin{frame}{Second Order Physics Constrained Learning}


In the vector form,
$$\boxed{\nabla^2 v = (\nabla G)^T \nabla^2 f (\nabla G) + \nabla^2 (\bar G^T G)\qquad \bar G = \nabla f}$$

\begin{itemize}
\item Consider a function composed of a  sequence of computations
$$v = \Phi_m(\Phi_{m-1}(\cdots (\Phi_1(z))))$$
\end{itemize}


\begin{minipage}{0.7\textwidth}
	\small
\begin{algorithmic}[1]
	\State Initialize $H \gets 0$
	\For{$k=m-1, m-2, \ldots, 1$}
	\State Define $f:= \Phi_m (\Phi_{m-1} (\cdots (\Phi_{k+1}(\cdot))))$, $G:= \Phi_k$
	\State Calculate the gradient (Jacobian) $J \gets \nabla G$
	\State Extract $\bar G$ from the saved gradient back-propagation data
	\State Calculate $Z = \nabla^2(\bar G^T G)$ \label{algo:second-order-algo-update-line}
	\State Update $H \gets J^THJ + Z$
	\EndFor
\end{algorithmic}
\end{minipage}~
\begin{minipage}{0.3\textwidth}
\begin{figure}
	\centering
	\includegraphics[width=0.6\textwidth]{figures/thesis/computeH.png}
\end{figure}
\end{minipage}


\end{frame}


\begin{frame}{Numerical Benchmark}
\begin{itemize}
\item We consider the heat equation in $\Omega = [0,1]^2$
\begin{equation*}
	\begin{aligned}
		\frac{\partial u}{\partial t} &= \nabla \cdot (\textcolor{red}{\kappa(x, y)} \nabla u))+  f(x, y) & x\in \Omega\\ 
		u(x,y,0) &= x(1-x)y^2(1-y)^2 & (x,y)\in \Omega\\
		u(x,y,t) &= 0 & (x,y)\in  \partial \Omega
	\end{aligned}
\end{equation*}

\item The diffusivity coefficient $\kappa$ and exact solution $u$ are given by 

\begin{equation*}
\begin{aligned}
	\kappa(x,y) &= 2x^2 - 1.05x^4 + x^6 +xy+y^2\\ 
	u(x,y,t) &= x(1-x)y^2(1-y)^2 e^{-t}
\end{aligned}
\end{equation*}

\item We learn a DNN approximation to $\kappa$ using full-field observations of $u$

$$\kappa(x,y) \approx \text{N}_\theta(x, y)$$
\end{itemize}
\end{frame}


\begin{frame}{Convergence}
	\begin{itemize}
		
		\item The optimization problem is given by 
		\begin{equation*}
			\min_\theta\; L(\theta) = \sum_n\sum_{i,j} \left(\frac{u_{i,j}^{n+1} - u_{i,j}^n}{\Delta t} - F_{i,j}( u^{n+1}; \theta) -  f^{n+1}_{i,j}\right)^2 
		\end{equation*}
		
		$F_{i,j}(u^{n+1}; \theta)$: the 4-point finite difference approximation to the Laplacian $\nabla\cdot (\text{N}_\theta \nabla u)$.
		
	
		\end{itemize}
	
		\begin{figure}[htbp]
		\centering
		\includegraphics[width=0.45\textwidth]{figures/thesis/losses2_dynamic.pdf}
	\end{figure}
	
\end{frame}

\begin{frame}{Effect of PDEs}
	$$\text{N}_\theta \rightarrow \text{(PDE Solver)}  \rightarrow \text{Loss Function}$$
	\vspace{-0.5cm}
	\begin{itemize}
\item Consider the loss function excluding the effects of PDEs 
$$    l(\theta) = \sum_{i,j} (\text{N}_\theta(x_{i,j}, y_{i,j}) - \kappa(x_{i,j}, y_{i,j}))^2 $$

\item Eigenvalue magnitudes of $\nabla^2 L(\theta)$ and $\nabla^2 l(\theta)$
	\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/thesis/pde_dnn.png}
\end{figure}
	\end{itemize}
\end{frame}

\begin{frame}{Effect of PDEs}
\begin{itemize}
\item Most of the eigenvalue directions at the local landscape of loss functions are ``flat'' $\Rightarrow$ ``effective degrees of freedom (DOFs)''. 

\item Physical constraints (PDEs) further cannibalize effective DOFs:

\

\begin{center}
		\begin{tabular}{@{}llll@{}}
		\toprule
		& BFGS & LBFGS & Trust Region \\ \midrule
		DNN-PDE     & \textbf{31}   & \textbf{22}    & \textbf{35}           \\
		DNN Only &  34   & 41    & 38           \\ \bottomrule
	\end{tabular}
\end{center}


\end{itemize}
\end{frame}

\begin{frame}{Effect of Widths and Depths}

\begin{itemize}
\item The ratio of zero eigenvalues \textbf{increases} as

\begin{itemize}
\item the number of hidden layers increase for a fixed number (20) of neurons per layer
\begin{center}
\begin{tabular}{@{}llll@{}}
	\toprule
	\# Hidden   Layers &  LBFGS & BFGS  & Trust Region \\ \midrule
	1                         & 76.54 & 72.84 & 77.78        \\
	2                        & 98.2  & 94.41 & 93.21        \\
	3                         & 98.7  & 98.15 & 96.09        \\ \bottomrule
\end{tabular}
\end{center}

\item the number of neurons per layer increases for a fixed number (3) of hidden layers
\begin{center}
	\begin{tabular}{@{}lllll@{}}
		\toprule
		\# Neurons per Layer & LBFGS & BFGS  & Trust Region \\ \midrule
		5                         & 93.83 & 85.19 & 69.14        \\
		10                         & 97.7  & 83.52 & 89.66        \\
		20                        & 96.2  & 97.39 & 96.42        \\ \bottomrule
	\end{tabular}
\end{center}
\end{itemize}
\end{itemize}

\end{frame}


\begin{frame}{Effect of Widths and Depths: conjecture}
\begin{itemize}
\item Implications for overparametrization: \textbf{the minimizer lies on a relatively higher dimensional manifold of the parameter space}.
\item Conjecture: overparameterization makes the optimization easier due to a larger chance of hitting the minimizer manifold.
\end{itemize}

	\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/thesis/min.png}
\end{figure}
\end{frame}

\begin{frame}{Summary}
	\begin{itemize}
\item Trust region methods converge significantly faster compared to first order/quasi second order methods by leveraging Hessian information. 

\item Second order physics constrained learning helps you calculate Hessian matrices efficiently. 

\item The local minimum of DNNs have small effective degrees of freedom compared to DNN sizes. 
	\end{itemize}
\end{frame}


\section{Conclusion}

\begin{frame}{Conclusion}
	$$\min_{{f}} L_h(u_h) \quad \mathrm{s.t.}\; F_h({f}, u_h) = 0$$
	
	\begin{quote}
		\textcolor{green}{\cmark} Develop algorithms and tools for solving inverse problems by \\ combining DNNs and numerical PDE solvers. 
	\end{quote}
	
	
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.9\textwidth]{figures/thesis/outline.pdf}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{A General Approach to Inverse Modeling}
	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/summary.png}
	\end{figure}
\end{frame}



\begin{frame}
\frametitle{Acknowledgement}
\vspace{-1.5cm}
\begin{figure}[hbt]
	\centering
	\includegraphics[width=0.8\textwidth,angle=270,origin=c]{figures/thesis/family.png}
\end{figure}

\end{frame}

\begin{frame}
	\LARGE
	\begin{center}
		Supporting Materials
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{Limitations and Future Work}
	\begin{itemize}
		\item Computational cost
		\begin{itemize}
			\item A PDE needs to be solved per inner iteration in the optimization process
			\item Calculating Hessians are very expensive: exploit the Hessian structure to accelerate computations
		\end{itemize}
		\item Convergence and accuracy of DNNs
		\item Ill-posed inverse problems
		\begin{itemize}
			\item Regularization 
			\item Bayesian approach
		\end{itemize}
		\item Robustness to noise 
		\item Theoretical investigations
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Major AD Frameworks}
	
	% Please add the following required packages to your document preamble:
	% \usepackage{booktabs}
	\footnotesize
	\begin{table}[]
		\begin{tabular}{@{}p{2cm}|p{2.5cm}|p{2cm}|p{3cm}@{}}
			\toprule
			& TensorFlow 1.x                                                                                             & PyTorch              & JAX                                                                                                                                                                                \\ \midrule
			Computational Graph & Static and Explicit                                                                                        & Dynamic and Explicit & Dynamic and Implicit                                                                                                                                                               \\ \hline
			Programming         & declarative                                                                                                & imperative           & imperative                                                                                                                                                                         \\\hline
			Focus               & Graph Optimization, AD                                                                                     & AD                   & AD                                                                                                                                                                                 \\\hline
			Computing           & CPU/GPU/TPU                                                                                                & CPU/GPU, TPU(-)      & CPU/GPU/TPU                                                                                                                                                                        \\\hline
			Highlight           & \begin{tabular}[c]{@{}p{2.5cm}@{}}$\bullet$ graph optimizations and manipulations\\$\bullet$ optimized tensor libraries\end{tabular} & intuitive APIs       & \begin{tabular}[c]{@{}p{3cm}@{}}$\bullet$ just-in-time compilation of Python functions into XLA-optimized kernels\\$\bullet$ arbitrary composition of pure functions\\$\bullet$ high order derivatives\end{tabular} \\ \bottomrule
		\end{tabular}
	\end{table}
\end{frame}

\begin{frame}
	\frametitle{AD Frameworks}
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.7\textwidth,angle=270,origin=c]{figures/thesis/adframeworks}
	\end{figure}
\end{frame}

\begin{frame}
\frametitle{Static Graph versus Dynamic Graph}
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
	\begin{tabular}{@{}p{1cm}|p{5cm}|p{5cm}@{}}
		\toprule
		& Static Graph                                                                                                                            & Dynamic Graph                                                                                                                             \\ \midrule
		Pros & \begin{tabular}[c]{@{}p{5cm}@{}}$\bullet$ graph optimizations, simplifications and rewriting;\\$\bullet$  easy to reason about and analyze the graph\end{tabular} &$\bullet$  intuitive: run to define.                                                                                                                  \\ \hline
		Cons & compiled-language-like: define to run.                                                                                                   & \begin{tabular}[c]{@{}p{5cm}@{}}$\bullet$ difficult to reason about and optimize;\\ $\bullet$ encourage trial and error instead of computations itself.\end{tabular} \\ \bottomrule
	\end{tabular}
\end{table}
\end{frame}

%}
%\usebackgroundtemplate{}
%----------------------------------------------------------------------------------------
%    PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

%------------------------------------------------



\end{document} 